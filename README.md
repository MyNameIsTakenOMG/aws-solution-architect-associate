# aws-solution-architect-associate
## Table of Contents
 - [AWS Cloud](#aws-cloud)
 - [AWS IAM](#aws-iam)
 - [EC2 Basics](#ec2-basics)
 - [EC2 Associate](#ec2-associate)
 - [EC2 Instance Storage](#ec2-instance-storage)
 - [High Availability and Scalability](#high-availability-and-scalability)
 - [RDS Aurora and ElastiCache](#rds-aurora-and-elasticache)
 - [Route53](#route53)
 - [Classic Solutions Architecture](#classic-solutions-architecture)
 - [S3](#s3)
 - [S3 Advanced](#s3-advanced)
 - [S3 Security](#s3-security)
 - [CloudFront and Global Accelerator](#cloudfront-and-global-accelerator)
 - [AWS Storage Extras](#aws-storage-extras)
 - [AWS Integration and Messaging](#aws-integration-and-messaging)
 - [Containers on AWS](#containers-on-aws)
 - [Serverless Overview](#serverless-overview)
 - [Serverless Architecture](#serverless-architecture)
 - [Databases in AWS](#databases-in-aws)
 - [Data and Analytics](#data-and-analytics)
 - [Machine Learning](#machine-learning)
 - [AWS Monitoring Audit and Performance](#aws-monitoring-audit-and-performance)
 - [AWS IAM Advanced](#aws-iam-advanced)
 - [AWS Security and Encryption](#aws-security-and-encryption)
 - [AWS VPC](#aws-vpc)
 - [Disaster Recovery and Migration](#disaster-recovery-and-migration)
 - [More Solutions Architecture](#more-solutions-architecture)
 - [Other services](#other-services)
 - [White Papers and Architectures](#white-papers-and-architectures)
 - [Filling Gaps](#filling-gaps)

## AWS Cloud
 - AWS Global Infrastructure:
   - AWS Regions: clusters of data centers (Most AWS services are region-scoped)
   - AWS Availability Zones
   - AWS Data Centers
   - AWS Edge Locations /Points of Presence
 - How to choose an AWS Region?
   - Compliance with data governance and legal requirements: data never leaves a region without your explicit permission
   - Proximity to customers: reduced latency
   - Available services within a Region: new services and new features aren’t available in every Region
   - Pricing: pricing varies from region to region and is transparent on the service pricing page
 - AWS Availability Zones: (min-3, max-6 per region, each AZ has one or more discrete data centers (separated))
 - AWS Points of Presence (Edge Locations): Content is delivered to end users with lower latency
 - services:
   - Global: IAM, Route53, Cloudfront, WAF
   - Region(most services): EC2(IaaS), Elastic Beanstalk(PaaS), Lambda(FaaS), Rekognition(SaaS)

## AWS IAM
 - Users and Groups: Root Account, Users and Groups(only contain users)
 - IAM-Permissions: JSON files called policies -- apply the privilege principle, don't give more permissions than a user needs.
 - IAM policies inheritance: users who belong to multiple groups can inherit multiple permissions
 - IAM policies structure: version(required), id(optional), statement(required)
   - statement: Sid(statement id--optional),Effect(allow/deny), Principal(to whom this policy applied to), Action(list of allow/deny), Resource(list of resources the actions applied to), Condition(conditions for when this policy takes effect)
 - IAM password policy
   - strong password = high security
   - password policy:
     - min length
     - specific char types
     - allow all iam users to change their passwords
     - password expiration
     - prevent password re-use
 - Multi-Factor Authentication - MFA: Password + MFA = Successful Login
 - How can users access AWS?
   - AWS Console (password + MFA)
   - AWS CLI(access key): alternative to AWS Console
   - AWS SDK(access key)
   - access key id & access key secret (don't share access keys)
 - IAM Roles for Services: an IAM role contains IAM permissions will be assigned to certain AWS services to perform some actions
 - IAM Security Tools:
   - IAM Credentials Report (account-level): a report that lists all your account's users and the status of their various credentials
   - IAM Access Advisor (user-level): Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies.
## EC2 Basics
 - EC2 = Elastic Compute Cloud = Infrastructure as a Service
 - EC2: renting virtual machine (EC2), storing data on virtual drives(EBS), distributing traffic across machines(ELB), Scaling (ASG)
 - EC2 sizing & configuration options:
   - OS: Linux, Windows, MacOS
   - CPU & RAM
   - storage: instance store(hardware), network-attached (EBS&EFS)
   - Network card: speed of the card, public IP
   - Firewall rules: security group
   - Bootstrap script (configure at first launch): EC2 User Data
     - launching commands when an EC2 instance starts (only first start, run once)
     - install software, updates, download files from internet or something else
     - user data script runs with root user
 - EC2 instance type:
   - naming convention: m5.2xlarge (m instance class, 5 generation, 2xlarge size within the instance class)
   - general purpose: great for a diversity of workloads such as web servers or code repositories, which strike the balance between compute, memory, and network
   - compute optimized: great for compute-intensive tasks that require high-performance CPUs: Batch processing, media transcoding, gaming server, machine learning.
   - memory optimized: great for processing large data sets in memory: real-time processing big unstructured data, BI, high-performance database
   - storage optimized: great for storage-intensive tasks that require high, sequential read and write access to large data sets on local storage: Data warehousing applications, High frequency online transaction processing (OLTP) systems.
 - Security Groups(stateful, NACL--stateless):  fundamental of network security in AWS,  control how traffic is allowed into or out of our EC2 Instances.
   - only contain rules
   - Security groups rules can reference by IP or by security group
     - ports
     - IP ranges - IPV4 / IPV6
     - inbound / outbound
   - can be attached to multiple instances
   - locked down to region/VPC
   - live outside of EC2
   - good to maintain a separated security group for SSH
 - Classic Ports: 22-SSH, 21-FTP, 22-Sftp, 80-HTTP, 443-HTTPS, 3389-RDP (Remote Desktop Protocol) – log into a Windows instance
 - SSH: one of the most important function. It allows you to control a remote machine, all using the command line.
 - EC2 Instance Connect: no need for access key file, open within a browser, a temporary key will be used (only works with aws linux2), and make sure to open port 22
 - EC2 Instances Purchasing Options:
   - On-Demand Instances – short-term and un-interrupted workload, predictable pricing, pay by second and for what you use
   - Reserved (1 & 3 years): 
     - Reserved Instances – long workloads, save up to 72% compared to on-demand , instance's scope -- regional or zonal, long-term and steady workloads, such as database
     - Convertible Reserved Instances – long workloads with flexible instances, save up to 66% compared to on-demand (can change instance type, family, OS, scope and tenancy)
   - Savings Plans (1 & 3 years): save up to 72%, locked down to instance family and region – a commitment to an amount of usage, long workload
   - Spot Instances: save up to 90%, most cost-effient – short workloads, cheap, can lose instances (less reliable), great for workloads that are resilient to failure(such as batch job, data analysis, any distributed job, image processing, etc), but not for critical jobs
   - Dedicated Hosts – book an entire physical server, control instance placement, allows to address compliance requirements and use existing server-bound software licenses. Most expensive, on-demand/reserved(1 or 3 yrs)
   - Dedicated Instances – no other customers will share your hardware, but may share hardware with other instances in the same account. The hardware is dedicated to you, no control over instance placement.
   - Capacity Reservations – reserve on-demand instances capacity in a **specific AZ** for any duration
 - Which purchasing option is right for me?
   - On-demand: coming and staying in resort whenever we like, we pay the full price
   - Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount.
   - Savings Plans: pay a certain amount per hour for certain period and stay in any room type (e.g., King, Suite, Sea View, …)
   - Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms. You can get kicked out at any time
   - Dedicated Hosts: We book an entire building of the resort
   - Capacity Reservations: you book a room for a period with full price even you don’t stay in it
 - EC2 Spot Instance Requests: define **max spot price** , when **current spot price** > **max spot price**, can choose to **stop** or **terminate** with 2 min grace period. Or **Spot Block** to **block** the instances during a specified time frame with interruptions.
 - How to terminate Spot Instances? you must first cancel a **spot request**, and then terminate the associated spot instances.
 - Spot Fleets: a set of spot instances + (optional)on-demand instances
   - trying to meet the target capacity with price constraints(define launch pools)
   - strategies to allocate spot instances: lowestPrice, diversified, capacityOptimized, priceCapacityOptimized(recommended)
   - Spot Fleets allow us to automatically request Spot Instances with the lowest price
## EC2 Associate
 - Private vs public IP (IPV4)
   - public IP: machine can be identified on the internet, meaning it's unique across the internet and can be geo-located easily.
   - private IP: machine can only be identified on a private network, meaning it's unique across the private network, connecting to public internet via NAT and internet gateway. Besides, only a specified range of IPs can be used as private IPs
 - Elastic IP: a fixed public IP for your EC2 instance, can only be attached to one instance at a time.
   - can only have up to 5 Elastic IPs in your account (ask aws to increase)
   - can quickly remap the address to another instance when another was failed
   - Overall, try to avoid using Elastic IP: 1)often reflect poor architectural decisions. 2)instead, use a random public IP with a registered DNS name. 3)or using a load balancer.
 - by default, each EC2 instance has a public IP and a private IP
 - Placement Groups:
   - strategies: 1)Cluster—clusters instances into a low-latency group in a single Availability Zone. 2)Spread—spreads instances across underlying hardware (max 7 instances per group per AZ). 3)Partition—spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)
 - Elastic Network Interfaces (ENI): Logical component in a VPC that represents a virtual network card. Can be created independently and attach them on the fly on EC2 instances for failover. ENI is bound to specific AZ.
   - Primary private IPv4, one or more secondary IPv4
   - One Elastic IP (IPv4) per private IPv4
   - One Public IPv4
   - One or more security groups
   - A MAC address
 - EC2 Hibernate: 1)the RAM state is written into root EBS(must be encrypted). 2)long-running processing, services that take time to initialize
   - the instance RAM size must be less then **150GB**
   - cannot be in the state of hibernate more than **60 days**
   - available for **on-demand, spot** instance
## EC2 Instance Storage
 - EBS Volume: An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run. It allows your instances to persist data, even after their termination(Root EBS will be terminated by default, can be turned off). It is bound to a specific availability zone(**AZ**). To move around, we need snapshot.
 - EBS – Delete on Termination attribute: by default, root EBS will be deleted, any other EBS will be kept. The behaviour of both types of EBS can be modified via console or CLI.
 - EBS Snapshots: Make a backup (snapshot) of your EBS volume at a point in time. Not necessary to detach volume to do snapshot, but recommended. Can copy snapshots across AZ or Region
 - EBS Snapshots Features:
   - EBS Snapshot Archive: 75% cheaper than normal snapshot. Takes within 24 to 72 hours for restoring the archive
   - Recycle Bin for EBS Snapshots: Setup rules to retain deleted snapshots so you can recover them after an accidental deletion. Specify retention (from 1 day to 1 year)
   - Fast Snapshot Restore (FSR): Force full initialization of snapshot to have no latency on the first use ($$$)
 - AMI Overview (Amazon Machine Image)
   - AMI are a customization of an EC2 instance
   - AMI are built for a specific region (and can be copied across regions):
     - Faster boot / configuration time because all your software is pre-packaged
     - You add your own software, configuration, operating system, monitoring…
   - You can launch EC2 instances from: A Public AMI(AWS provided), Your own AMI(you make and maintain them yourself), An AWS Marketplace AMI: an AMI someone else made (and potentially sells)
 - AMI Process (from an EC2 instance)
   - Start an EC2 instance and customize it
   - Stop the instance (for data integrity)
   - Build an AMI – this will also create EBS snapshots(behind the scenes)
   - Launch instances from other AMIs
 - EC2 Instance Store
   - EBS volumes are network drives with good but “limited” performance
   - If you need a **high-performance hardware disk**, use EC2 Instance Store
   - great I/O, EC2 Instance Store lose their storage if they’re stopped (ephemeral). Good for buffer / cache / scratch data / temporary content.Risk of data loss if hardware fails. Backups and Replication are your responsibility.
 - EBS Volume Types
   - EBS Volumes come in 6 types
     - gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads
       - use case: 1GB--16TB, gp3: baseline (3000I/O, 125MB/s), up to 16000I/O, 1000MB/s independently. gp2: burst to 3000I/O, size and I/O linked, max 16000I/O
     - io1 / io2 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads
       - use case: more than 16000I/O, Great for databases workloads (sensitive to storage perf and consistency), io1/io2 (4 GiB - 16 TiB): size and I/O independent, max 64000I/O for Nitro & 32000 for other. io2 Block Express (4 GiB – 64 TiB):Sub-millisecond latency, Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1. **Supports EBS Multi-attach**
     - st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads
     - sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads
       - use case(HDD): Cannot be a boot volume. 125 GiB to 16 TiB. st1: Max throughput 500 MiB/s – max IOPS 500, Big Data, Data Warehouses, Log Processing. sc1: Max throughput 250 MiB/s – max IOPS 250, Scenarios where lowest cost is important
   - EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)
   - **Only gp2/gp3 and io1/io2 can be used as boot volumes**
 - EBS Multi-Attach – io1/io2 family
   - Attach the same EBS volume to multiple EC2 instances in the same AZ
   - Each instance has full read & write permissions to the high-performance volume
   - Up to 16 EC2 Instances at a time
   - Use case: Achieve higher application availability in clustered Linux applications (ex: Teradata). Applications must manage concurrent write operations
   - Must use a file system that’s cluster-aware (not XFS, EXT4, etc…)
 - EBS Encryption
   - Encryption and decryption are handled transparently (you have nothing to do)
   - Encryption has a minimal impact on latency
   - EBS Encryption leverages keys from KMS (AES-256)
   - **Copying an unencrypted snapshot allows encryption**
   -  encrypted EBS volume: data at rest, data in flight , snapshots, volumes from snapshots are all encrypted
   -  Snapshots of encrypted volumes are encrypted
 - Encryption: encrypt an unencrypted EBS volume: create a snapshot from an unencrypted EBS, then using **copy** operation to encrypt the snapshot, then create an EBS from the snapshot.
 - Amazon EFS – Elastic File System
   - Managed NFS (network file system) that can be mounted on many EC2
   - EFS works with EC2 instances in multi-AZ
   - Highly available, scalable, expensive (3x gp2), pay per use
   - Use cases: content management, web serving, data sharing, Wordpress
   - Uses NFSv4.1 protocol, and use security group to control access to EFS
   - **Compatible with Linux based AMI (not Windows)**
   - Encryption at rest using KMS
   - POSIX file system (~Linux) that has a standard file API
   - File system scales automatically, pay-per-use, no capacity planning!
 - EFS – Performance & Storage Classes
   - EFS Scale: 1000s of concurrent NFS clients, 10 GB+ /s throughput, Grow to Petabyte-scale network file system, automatically
   - Performance Mode (set at EFS creation time): General Purpose (default)--web server, CMS,etc... Max I/O--big data, media processing
   - Throughput Mode: Bursting , Provisioned(regardless size), Elastic
   - Storage Tiers (lifecycle management feature– move file after N days): Standard, Infrequent access (EFS-IA)
   - Availability and durability: Standard--multi-AZ, One Zone -- could save 90% cost
## High Availability and Scalability
 - Load Balances: are servers that forward traffic to multiple servers (e.g., EC2 instances) downstream
   - Spread load across multiple downstream instances
   - Expose a single point of access (DNS) to your application
   - Do regular health checks to your instances. Seamlessly handle failures of downstream instances
   - Provide SSL termination (HTTPS) for your websites
   - Enforce stickiness with cookies
   - High availability across zones
   - Separate public traffic from private traffic
   - An Elastic Load Balancer is a managed load balancer
 - Health Checks
   - They enable the load balancer to know if instances it forwards traffic to are available to reply to requests
   - The health check is done on a port and a route (/health is common)
   - If the response is not 200 (OK), then the instance is unhealthy
 - Types of load balancer on AWS
   - Classic Load Balancer (deprecated)
   - Application Load Balancer (layer 7 http, https)
   - Network Load Balancer (layer 4 tcp , udp)
   - Gateway Load Balancer (layer 3 ip procotol)
   - **Some load balancers can be setup as internal (private) or external (public) ELBs**
 - Load Balancer Security Groups: the EC2 instances should have the security group **source** of ELB security group
 - Application Load Balancer (Layer 7 http)
   - target groups: Ec2 instances, containers
   - support Http/2 and websocket
   - support redirect (http --> https)
   - Routing tables to different target groups
     - based on URL path
     - based on URL hostname
     - based on query string or headers
     - ALB are a great fit for micro services & container-based application
     - Has a port mapping feature to redirect to a dynamic port in ECS(example: Docker & Amazon ECS)
   - Target Groups
     - EC2 instances
     - ECS tasks
     - Lambda functions
     - IP Addresses – must be private IPs
     - ALB can route to multiple target groups
     - **Health checks are at the target group level**
   - **fixed hostname**
   - The application servers don’t see the IP of the client directly (through the header X-Forwarded-For)
 - Network Load Balancer 
   - Forward TCP & UDP traffic to your instances
   - Handle millions of request per seconds, less latency
   - **NLB has one static IP per AZ, and supports assigning Elastic IP**
   - Not included in the AWS free tier
   - used for extreme performance, TCP or UDP traffic
   - Target Groups
     - EC2 instances
     - IP Addresses – must be private IPs
     - Application Load Balancer
     - Health Checks support the TCP, HTTP and HTTPS Protocols
 - Gateway Load Balancer
   - Deploy, scale, and manage a fleet of 3 network virtual appliances in AWS
   - Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, …
   - Operates at Layer 3 (Network Layer) – IP Packets
   - Uses the GENEVE protocol on port 6081
   - Transparent Network Gateway – **single entry/exit** for all traffic & Load Balancer – distributes traffic to your virtual appliances
   - Target Groups: EC2 instances and IP Addresses – must be private IPs
 - Sticky Sessions (Session Affinity)
   - It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer
   - this works for CLB, ALB, NLB (not GWLB)
   - Enabling stickiness may bring imbalance to the load over the backend EC2 instances
   - For both CLB & ALB, the “cookie” used for stickiness has an expiration date you control
   - Cookie Names:
     - Application-based Cookies
       - Custom cookie: Generated by the target, Don’t use **AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)**
       - Application cookie: Generated by the load balancer. Cookie name is **AWSALBAPP**
     - Duration-based Cookies
       - Cookie generated by the load balancer
       - Cookie name is **AWSALB** for ALB, **AWSELB** for CLB
 - Cross-Zone Load Balancing: each load balancer instance distributes evenly across all registered instances in all AZ. Otherwise, each load balancer only handle traffic in its own AZ evenly.
   - ALB: Enabled by default (can be disabled at the Target Group level), No charges for inter AZ data
   - NLB & GWLB: Disabled by default, not free
   - CLN: Disabled by default, No charges for inter AZ data if enabled
 - SSL/TLS - Basics
   - An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)
   - SSL refers to Secure Sockets Layer; TLS refers to Transport Layer Security, which is a newer version. Nowadays, TLS certificates are mainly used, but people still refer as SSL
   - Public SSL certificates are issued by Certificate Authorities (CA)
   - SSL certificates have an expiration date (you set) and must be renewed
 - Load Balancer - SSL Certificates
   - You can manage certificates using ACM (AWS Certificate Manager)
   - The load balancer uses an X.509 certificate (SSL/TLS server certificate)
   - for HTTP listeners:
     - You must specify a default certificate
     - You can add an optional list of certs to support multiple domains
     - Clients can use **SNI** (Server Name Indication) to specify the hostname they reach
     - Ability to specify a security policy to support older versions of SSL / TLS (legacy clients)
 - SSL – Server Name Indication (SNI): Only works for ALB & NLB (newer generation), CloudFront, not CLB
   - SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)
   - It’s a “newer” protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake
   - The server will then find the correct certificate, or return the default one
 - Elastic Load Balancers – SSL Certificates
   - Application Load Balancer: support multiple listeners with multiple SSL certificates. Uses Server Name Indication (SNI) to make it work
   - Network Load Balancer: Supports multiple listeners with multiple SSL certificates. Uses Server Name Indication (SNI) to make it work
   - Classic Load Balancer: Support only one SSL certificate
 - Connection Draining:
   - Feature naming
     - Connection Draining – for CLB
     - Deregistration Delay – for ALB & NLB
   - Time to complete “in-flight requests” while the instance is de-registering or unhealthy.
   - Stops sending new requests to the EC2 instance which is de-registering
   - Can be disabled (set value to 0): Between 1 to 3600 seconds (default: 300 seconds)
   - Set to a low value if your requests are short
 - Auto Scaling Group (free, but have to pay for underlying EC2 instances)
   - auto scaling, min and max EC2 instances, Automatically register new instances to a load balancer, health check
 - Auto Scaling Group Attributes:
   - Launch Template:
     - AMI + Instance Type
     - EC2 User Data
     - EBS Volumes
     - Security Groups
     - SSH Key Pair
     - IAM Roles for your EC2 Instances
     - Network + Subnets Information
     - Load Balancer Information
   - Min Size / Max Size / Initial Capacity
   - Scaling Policies
 - Auto Scaling - CloudWatch Alarms & Scaling
   - also ELB can pass health check to ASG
   - It is possible to scale an ASG based on CloudWatch alarms
   - Based on the alarm, we can create scaling policies (scale-out or scale-in)
 - Auto Scaling Groups – Dynamic Scaling Policies
   - Target Tracking Scaling: Most simple and easy to set-up
   - Simple / Step Scaling: based on CloudWatch alarm to scale the group
   - Scheduled Actions: Anticipate a scaling based on known usage patterns
 - Auto Scaling Groups – Predictive Scaling
   - Predictive scaling: continuously forecast load and schedule scaling ahead
 - Good metrics to scale on: CPUUtilization, RequestCountPerTarget, Average Network In / Out (if you’re application is network bound), Any custom metric (that you push
using CloudWatch)
 - Auto Scaling Groups - Scaling Cooldowns
   - After a scaling activity happens, you are in the cooldown period (default 300 seconds)
   - During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize)
   - Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period

## RDS Aurora and ElastiCache
 - Amazon RDS Overview: RDS stands for Relational Database Service
   - supports: Postgres, MySQL, MariaDB, Oracle, Microsoft SQL Server, Aurora (AWS Proprietary database)
   - Advantage over using RDS versus deploying DB on EC2
     - RDS is a managed service:
       - Automated provisioning, OS patching
       - Continuous backups and restore to specific timestamp (Point in Time Restore)!
       - Monitoring dashboards
       - Read replicas for improved read performance
       - Multi AZ setup for DR (Disaster Recovery)
       - Scaling capability (vertical and horizontal)
       - Maintenance windows for upgrades
     - BUT you can’t SSH into your instances
 - RDS – Storage Auto Scaling
   - You have to set Maximum Storage Threshold (maximum limit for DB storage)
   - Automatically modify storage if: 1)Free storage is less than 10% of allocated storage. 2)Low-storage lasts at least 5 minutes. 3)6 hours have passed since last modification
   - Useful for applications with unpredictable workloads
 - RDS Read Replicas for read scalability
   - Up to 15 Read Replicas
   - Within AZ, Cross AZ or Cross Region
   - Replication is **ASYNC**, so reads are eventually consistent
   - Replicas can be promoted to their own DB
   - `Applications must update the connection string to leverage read replicas`
   - Use Cases: a RDS read replica for analysis purposes, while the main RDS instance will not be affected.
 - RDS Read Replicas – Network Cost
   - In AWS there’s a network cost when data goes from one AZ to another
   - For RDS Read Replicas within the same region, you don’t pay that fee
 - RDS Multi AZ (Disaster Recovery)
   - **SYNC** replication
   - One DNS name – automatic app failover to standby
   - Not used for scaling
   - No manual intervention in appsFailover in case of loss of AZ, loss of network, instance or storage failure
   - **Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)**
 - RDS – From Single-AZ to Multi-AZ
   - Zero downtime operation (no need to stop the DB)
   - Just click on “modify” for the database. The following happens internally: A snapshot is taken. A new DB(standby DB) is restored from the snapshot in a new AZ. Synchronization is established between the two databases
 - RDS Custom: `Managed Oracle and Microsoft SQL Server Database with OS and database customization`
   - access to the underlying database and OS so you can Configure settings, Install patches, Enable native features, Access the underlying EC2 Instance using SSH or SSM Session Manager
   - `De-activate Automation Mode to perform your customization, better to take a DB snapshot before`
   - RDS Custom: full admin access to the underlying OS and the database
 - Amazon Aurora
   - Failover in Aurora is automatically instantaneous. It’s HA (High Availability) native.
   - Aurora is a proprietary technology from AWS (not open sourced)
   - Postgres and MySQL are both supported as Aurora DB
   - Aurora is “AWS cloud optimized” and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS
   - Aurora storage automatically grows in increments of 10GB, up to 128 TB.
   - Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag)
   - Aurora costs more than RDS (20% more) – but is more efficient
   - Backup and Recovery
   - Isolation and security
   - Industry compliance
   - Push-button scaling
   - Automated Patching with Zero Downtime
   - Advanced Monitoring
   - Routine Maintenance
   - Backtrack: restore data at any point of time without using backups
 - Aurora High Availability and Read Scaling
   - 6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes. 3 copies out of 6 need for reads. Self healing with peer-to-peer replication. Storage is striped across 100s of volumes
   - One Aurora Instance takes writes (master)
   - Automated failover for master in less than 30 seconds
   - Master + up to 15 Aurora Read Replicas serve reads
   - Support for Cross Region Replication
 - Aurora DB Cluster
   - Writer Endpoint Pointing to the master
   - Reader Endpoint Connection Load Balancing
   - Custom Endpoints:
     - Define a subset of Aurora Instances as a Custom Endpoint
     - Example: Run analytical queries on specific replicas
     - The Reader Endpoint is generally **not** used after defining Custom Endpoints
 - Aurora Replicas - Auto Scaling: based on certain metrics of existing Aurora instances, such as CPU usage
 - Aurora Serverless:
   - Automated database instantiation and auto-scaling based on actual usage (quickly auto-scaling)
   - Good for infrequent, intermittent or unpredictable workloads
   - No capacity planning needed
   - Pay per second, can be more cost-effective
 - Aurora Multi-Master (similar to EFS, probably need concurrency control)
   - In case you want continuous write availability for the writer nodes
   - Every node does R/W - vs promoting a Read Replica as the new master
 - Global Aurora
   - Aurora Cross Region Read Replicas: Useful for disaster recovery, Simple to put in place
   - Aurora Global Database (recommended):
     - 1 Primary Region (read / write)
     - Up to 5 secondary (read-only) regions, replication lag is less than 1 second
     - Up to 16 Read Replicas per secondary region
     - Helps for decreasing latency
     - Promoting another region (for disaster recovery) has an RTO of < 1 minute
     - Typical cross-region replication takes less than 1 second
 - Aurora Machine Learning: Enables you to add ML-based predictions to your applications via SQL
   - Simple, optimized, and secure integration between Aurora and AWS ML services
   - Supported services: Amazon SageMaker (use with any ML model), Amazon Comprehend (for sentiment analysis)
   - You don’t need to have ML experience
   - Use cases: fraud detection, ads targeting, sentiment analysis, product recommendations
 - RDS Backups
   - Automated backups: Daily full backup of the database (during the backup window). Transaction logs are backed-up by RDS every 5 minutes. => ability to restore to any point in time (from oldest backup to 5 minutes ago). 1 to 35 days of retention, set 0 to disable automated backups
   - Manual DB Snapshots: Manually triggered by the user. Retention of backup for as long as you want
   - Trick: in a stopped RDS database, you will still pay for storage. If you plan on stopping it for a long time, you should snapshot & restore instead
 - Aurora Backups
   - Automated backups: 1 to 35 days (cannot be disabled), point-in-time recovery in that timeframe
   - Manual DB Snapshots: Manually triggered by the user. Retention of backup for as long as you want
 - RDS & Aurora Restore options
   - Restoring a RDS / Aurora backup or a snapshot creates a new database
   - Restoring MySQL RDS database from S3: Create a backup of your on-premises database, store it in the S3, then restore a new RDS from the backup.
   - Restoring MySQL Aurora cluster from S3: Create a backup of your on-premises database using `Percona XtraBackup`, store it in the S3, then restore a new RDS from the backup.
 - Aurora Database Cloning
   - Create a new Aurora DB Cluster from an existing one
   - Faster than snapshot & restore
   - Uses copy-on-write protocol: Initially, the new DB cluster uses the same data volume as the original DB cluster (fast and efficient– no copying is needed). When updates are made to the new DB cluster data, then additional storage is allocated and data is copied to be separated
   - Very fast & cost-effective
   - **Useful to create a “staging” database from a “production” database without impacting the production database**
 - RDS & Aurora Security
   - At-rest encryption: Database master & replicas encryption using AWS KMS – must be defined as launch time. If the master is not encrypted, the read replicas cannot be encrypted. To encrypt an un-encrypted database, go through a DB snapshot & restore as encrypted
   - In-flight encryption: TLS-ready by default, use the AWS TLS root certificates client-side
   - IAM Authentication: IAM roles to connect to your database (instead of username/pw)
   - Security Groups: Control Network access to your RDS / Aurora DB
   - No SSH available except on RDS Custom
   - Audit Logs can be enabled and sent to CloudWatch Logs for longer retention
 - Amazon RDS Proxy: Fully managed database proxy for RDS. Allows apps to pool and share DB connections established with the database
   - `Improving database efficiency by reducing the stress on database resources (e.g., CPU, RAM) and minimize open connections (and timeouts)`
   - Serverless, autoscaling, highly available (multi-AZ)
   - `Reduced RDS & Aurora failover time by up 66%`
   - Supports RDS (MySQL, PostgreSQL, MariaDB, MS SQL Server) and Aurora (MySQL, PostgreSQL)
   - No code changes required for most apps
   - `Enforce IAM Authentication for DB, and securely store credentials in AWS Secrets Manager
   - `RDS Proxy is never publicly accessible (must be accessed from VPC)`
 - Amazon ElastiCache Overview: The same way RDS is to get managed Relational Databases…
   - ElastiCache is to get managed Redis or Memcached
   - Caches are in-memory databases with really high performance, low latency
   - Helps reduce load off of databases for read intensive workloads
   - Helps make your application stateless
   - AWS takes care of OS maintenance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups
   - Using ElastiCache involves heavy application code changes
 - ElastiCache Solution Architecture - DB Cache
   - Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache.
   - Helps relieve load in RDS
   - **Cache must have an invalidation strategy to make sure only the most current data is used in there.**
 - ElastiCache Solution Architecture – User Session Store
   - User logs into any of the application
   - The application writes the session data into ElastiCache
   - The user hits another instance of our application
   - The instance retrieves the data and the user is already logged in
 - ElastiCache – Redis vs Memcached
   - REDIS:
     - Multi AZ with Auto-Failover
     - Read Replicas to scale reads and have high availability
     - Data Durability using AOF persistence
     - Backup and restore features
     - Supports Sets and Sorted Sets
   - MEMCACHED:
     - Multi-node for partitioning of data (sharding)
     - No high availability (replication)
     - Non persistent
     - No backup and restore
     - Multi-threaded architecture
 - ElastiCache – Cache Security
   - ElastiCache supports IAM Authentication for Redis
   - IAM policies on ElastiCache are only used for AWS API-level security
   - Redis AUTH:
     - You can set a “password/token” when you create a Redis cluster
     - This is an extra level of security for your cache (on top of security groups)
     - Support SSL in flight encryption
   - Memcached: Supports SASL-based authentication (advanced)
 - Patterns for ElastiCache
   - Lazy Loading: all the read data is cached, data can become stale in cache
   - Write Through: Adds or update data in the cache when written to a DB (no stale data)
   - Session Store: store temporary session data in a cache (using TTL features)
   - Quote: There are only two hard things in Computer Science: cache invalidation and naming things
 - ElastiCache – Redis Use Case
   - Gaming real-time Leaderboards are computationally complex
   - Redis Sorted sets guarantee both uniqueness and element ordering
   - Each time a new element added, it’s ranked in real time, then added in correct order

## Route53
 - What is DNS? Domain Name System which translates the human friendly hostnames into the machine IP addresses
 - DNS Terminologies:
   - Domain Registrar: Amazon Route 53, GoDaddy, …
   - DNS Records: A, AAAA, CNAME, NS, …
   - Zone File: contains DNS records
   - Name Server: resolves DNS queries (Authoritative or Non-Authoritative)
   - Top Level Domain (TLD): .com, .us, .in, .gov, .org, …
   - Second Level Domain (SLD): amazon.com, google.com, …
 - Amazon Route 53
   - A highly available, scalable, fully managed and Authoritative DNS: Authoritative = the customer (you) can update the DNS records
   - Route 53 is also a Domain Registrar
   - `Ability to check the health of your resources`
   - The only AWS service which provides 100% availability SLA
   - Why Route 53? 53 is a reference to the traditional DNS port
 - Route 53 – Records: How you want to route traffic for a domain
   - Each record contains:
     - Domain/subdomain Name – e.g., example.com
     - Record Type – e.g., A or AAAA
     - Value – e.g., 12.34.56.78
     - Routing Policy – how Route 53 responds to queries
     - TTL – amount of time the record cached at DNS Resolvers
   - Route 53 supports the following DNS record types:
     - (must know)
       - A: maps a hostname to IPv4
       - AAAA: maps a hostname to IPv6
       - CNAME: maps a hostname to another hostname
         - The target is a domain name which must have an A or AAAA record
         - Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)
         - Example: you can’t create for example.com, but you can create for www.example.com
       - NS: Name Servers for the Hosted Zone: Control how traffic is routed for a domain
     - (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV
 - Route 53 – Hosted Zones (You pay $0.50 per month per hosted zone)
   - A container for records that define how to route traffic to a domain and its subdomains
   - Public Hosted Zones – contains records that specify how to route traffic on the Internet (public domain names)
   - Private Hosted Zones – contain records that specify how you route traffic `within` one or more VPCs (private domain names)
 - Route 53 – Records TTL (Time To Live)
   - High TTL – e.g., 24 hr: Less traffic on Route 53. Possibly outdated records
   - Low TTL – e.g., 60 sec.: More traffic on Route 53 ($$). Records are outdated for less time. Easy to change records.
   - Except for Alias records, TTL is mandatory for each DNS record
 - CNAME vs Alias
   - CNAME: Points a hostname to any other hostname. ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)
   - Alias: Points a hostname to an AWS Resource. Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com). `Free of charge`. `Native health check`
     - Maps a hostname to an AWS resource
     - An extension to DNS functionality
     - Automatically recognizes changes in the resource’s IP addresses
     - Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com
     - Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)
     - `You can’t set the TTL`
     - Targets: ELB, CloudFront, API gateway, S3 websites, Elastic Beanstalk environments, VPC Interface Endpoints, Global Accelerator accelerator, Route 53 record in the same hosted zone. **You cannot set an ALIAS record for an EC2 DNS name**
 - Route 53 – Routing Policies: Define how Route 53 responds to DNS queries. **NOTE:** unlike ELB routing the incoming traffic, it only responses the DNS queries.
   - Routing Policies:
     - Simple: Typically, route traffic to a single resource. But it can specify multiple values for the same record. **If multiple values are returned, a random one is chosen by the client**. When Alias enabled, specify only one AWS resource . Can’t be associated with Health Checks
     - Weighted: Control the % of the requests that go to each specific resource. DNS records must have the same name and type. Can be associated with Health Checks. `Assign a weight of 0 to a record to stop sending traffic to a resource. If all records have weight of 0, then all records will be returned equally`. Use cases: load balancing between regions, testing new application versions…
     - Failover: active-passive pattern (Secondary EC2 instance for example – Disaster Recovery)
     - Latency based: Redirect to the resource that has the least latency close to us. Super helpful when latency for users is a priority. **Latency is based on traffic
between users and AWS Regions**. Can be associated with Health Checks (has a failover capability)
     - Geolocation: Different from Latency-based!
       - `This routing is based on user location`
       - Specify location by Continent, Country or by US State
       - Should create a “Default” record (in case there’s no match on location)
       - Can be associated with Health Checks
       - Use cases: website localization, restrict content distribution, load balancing, …
     - Multi-Value Answer: Use when routing traffic to multiple resources
       - Route 53 return multiple values/resources
       - Can be associated with Health Checks (return only values for healthy resources)
       - Up to 8 healthy records are returned for each Multi-Value query
       - `Multi-Value is not a substitute for having an ELB`
     - IP-based Routing: Routing is based on clients’ IP addresses
       - You provide a list of CIDRs for your clients and the corresponding endpoints/locations (user-IP-to-endpoint mappings)
       - Example: route end users from a particular ISP to a specific endpoint
     - Geoproximity (using Route 53 **Traffic Flow** feature): Route traffic to your resources based on the geographic location of users and resources
       - `Ability to shift more traffic to resources based on the defined bias`
       - To change the size of the geographic region, specify bias values: To expand (1 to 99) – more traffic to the resource. To shrink (-1 to -99) – less traffic to the resource
       - Resources can be: AWS resources (specify AWS region). Non-AWS resources (specify Latitude and Longitude)
 - Route 53 – Health Checks
   - HTTP Health Checks are only for **public resources**
   - Health Check => Automated DNS Failover:
     - Health checks that monitor an endpoint
     - Health checks that monitor other health checks (Calculated Health Checks)
     - Health checks that monitor CloudWatch Alarms (full control !!) --  (helpful for private resources)
   - Health Checks are integrated with CW metrics
   - Monitor an Endpoint
     - About 15 global health checkers will check the endpoint health: Healthy/Unhealthy Threshold – 3 (default). Interval – 30 sec (can set to 10 sec – higher cost). Supported protocol: HTTP, HTTPS and TCP. If > 18% of health checkers report the endpoint is healthy, Route 53 considers it Healthy. Otherwise, it’s Unhealthy. Ability to choose which locations you want Route 53 to use.
     - Health Checks pass only when the endpoint responds with the 2xx and 3xx status codes
     - Health Checks can be setup to pass / fail based on the text in the first 5120 bytes of the response
     - Configure you router/firewall to allow incoming requests from Route 53 Health Checkers
 - Route 53 – Calculated Health Checks
   - Combine the results of multiple Health Checks into a single Health Check using `AND`, `OR`, `NOT`
   - Can monitor up to 256 Child Health Checks
   - Specify how many of the health checks need to pass to make the parent pass
   - Usage: perform maintenance to your website without causing all health checks to fail
 - Health Checks – Private Hosted Zones
   - Route 53 health checkers are outside the VPC
   - They can’t access private endpoints (private VPC or on-premises resource)
   - You can create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm itself CloudWatch monitor
 - Domain Registar vs. DNS Service
   - You buy or register your domain name with a Domain Registrar typically by paying annual charges . The Domain Registrar usually provides you with a DNS service to manage your DNS records. But you can use another DNS service to manage your DNS records. Example: purchase the domain from GoDaddy and use Route 53 to manage your DNS records
   - Create a Hosted Zone in Route 53
   - Update NS Records on 3rd party website to use Route 53 Name Servers
   - Route53 is a DNS registar and DNS service
## Classic Solutions Architecture
 - stateless web app(maybe some api endpoints): Route53 -- client -- ELBs(ALB):Multi-AZ -- ASG(multi-az): EC2 reserved capacity (cost-saving)
   - cost: ASG to ensure right amount of EC2 instances, and reserved capacity instances
   - performance: ELB and ASG
   - reliability: Route53 reliably 'direct' traffic to the ELB
   - security: security group for EC2 instances
   - operational experience: from manually managing EC2 instances to automatically managing EC2 using ASG
 - stateful web app(such as online shopping web):
   - Route53 -- client -- ELB: multi-az -- ASG(multi-az): EC2 instances -- ElastiCache(multi-az) (store session data or cache query data(lazy-loading)) + RDS(multi-az,read replicas) /DynamoDB/...
   - security groups: elastiCache or DBs secruity groups restricted from EC2 security groups
 - stateful web app(such as wordpress web):
   - we can use Aurora multi-az to go very scalable and available, and as for storing images, we can choose EFS or S3(more preferred nowadays)
 - Instantiating Applications quickly
   - EC2 Instances:
     - Use a Golden AMI: Install your applications, OS dependencies etc.. beforehand and launch your EC2 instance from the Golden AMI
     - Bootstrap using User Data: For dynamic configuration, use User Data scripts
     - Hybrid: mix Golden AMI and User Data (Elastic Beanstalk)
   - RDS Databases: Restore from a snapshot: the database will have schemas and data ready!
   - EBS Volumes: Restore from a snapshot: the disk will already be formatted and have data!
 - Elastic Beanstalk – Overview
   - It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, …
   - We still have full control over the configuration
   - Managed service:
     - Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, …
     - only application code is the responsibility for the developer
 - Elastic Beanstalk – Components
   - Application: collection of Elastic Beanstalk components (environments, versions, configurations, …)
   - Application Version: an iteration of your application code
   - Environment: Collection of AWS resources running an application version (only one application version at a time)
     - Tiers: Web Server Environment Tier & Worker Environment Tier
     - You can create multiple environments (dev, test, prod, …)

## S3
 - one of the main building blocks of AWS. Infinitely scaling. Can host website and integrate with other AWS services.
 - use cases: Backup and storage, Disaster Recovery, Application hosting, Data lakes & big data analytics, Static website, Software delivery...
 - store files in buckets(directories), which must have a global unique name(across all regions, all accounts).
 - buckets that defined at the region level
 - S3 is not a global service
 - bucket -- naming conventions include: Must NOT start with the prefix `xn--`, Must NOT end with the suffix `-s3alias`, No uppercase, No underscore, Must start with lowercase letter or number, 3-63 characters long, Not an IP.
 - objects: Objects (files) have a Key, is the FULL path and  composed of prefix + object name (There’s no concept of “directories” within buckets, Just keys with very long names that contain slashes (“/”))
 - objects (cont.): Object values are the content of the body(Max. Object Size is 5TB (5000GB), If uploading more than 5GB, must use “multi-part upload”). Metadata , Tags(useful for security / lifecycle) ,Version ID (if versioning is enabled)
 - Security:
   - User-based: IAM user policies
   - Resource-Based: Bucket Policies / Object Access Control List (ACL) / Bucket Access Control List (ACL)
   - **note:** an IAM principal can access an S3 object if the user IAM permissions ALLOW it OR the resource policy ALLOWS it AND there’s no explicit DENY
   - encryption: encrypt objects in Amazon S3 using encryption keys
 - S3 Bucket Policies: JSON based policies (Resources, effect, actions, principals)
 - Amazon S3 – Static Website Hosting: S3 can host static websites and have them accessible on the Internet. If you get a 403 Forbidden error, make sure the bucket policy allows public reads!
 - Amazon S3 -Versioning (It is enabled at the bucket level)
   - It is best practice to version your buckets (Protect against unintended deletes (ability to restore a version), Easy roll back to previous version)
   - Notes: Any file that is not versioned prior to enabling versioning will have version “null”. Suspending versioning does not delete the previous versions
 - Amazon S3 – Replication (Cross-Region Replication CRR & Same-Region Replication SRR): Must enable Versioning in source and destination buckets. Must give proper IAM permissions to S3. Copying is asynchronous. Buckets can be in different AWS accounts
   - After you enable Replication, only new objects are replicated
   - Optionally, you can replicate existing objects using S3 Batch Replication (Replicates existing objects and objects that failed replication)
   - For DELETE operations (Can replicate delete markers from source to target (optional setting), Deletions with a version ID are not replicated (to avoid malicious deletes))
   - There is no “chaining” of replication (eg. objects in bucket 1 will not be replicated into bucket 3)
 - S3 Storage Classes:
   - S3 Standard - General Purpose: Low latency and high throughput, Used for frequently accessed data (Use Cases: Big Data analytics, mobile & gaming applications, content
distribution…)
   - S3 Standard-Infrequent Access (IA): For data that is less frequently accessed, but requires rapid access when needed (Disaster Recovery, backups)
   - S3 One Zone-Infrequent Access: Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate (data lost when AZ destroyed)
   - S3 Glacier Instant Retrieval: Minimum storage duration of 90 days, Millisecond retrieval
   - S3 Glacier Flexible Retrieval: Minimum storage duration of 90 days, Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
   - S3 Glacier Deep Archive: Minimum storage duration of 180 days, Standard (12 hours), Bulk (48 hours)
   - S3 Intelligent Tiering: no retrieval charges, Small monthly monitoring and auto-tiering fee, Moves objects automatically between Access Tiers based on usage
     - Frequent Access tier (automatic): default tier
     - Infrequent Access tier (automatic): objects not accessed for 30 days
     - Archive Instant Access tier (automatic): objects not accessed for 90 days
     - Archive Access tier (optional): configurable from 90 days to 700+ days
     - Deep Archive Access tier (optional): config. from 180 days to 700+ days
   - Can move between classes manually or using S3 Lifecycle configurations:
 - S3 Durability and Availability
   - Durability: High durability (99.999999999%, 11 9’s) of objects across multiple AZ (Same for all storage classes)
   - Availability:Varies depending on storage class
## S3 Advanced
 - Amazon S3 – Moving between Storage Classes: You can transition objects between storage classes, Moving objects can be automated using a Lifecycle Rules
 - Amazon S3 – Lifecycle Rules: Rules can be created for a certain prefix, Rules can be created for certain objects Tags 
   - Transition Actions – configure objects to transition to another storage class
   - Expiration actions – configure objects to expire (delete) after some time (Can be used to delete old versions of files (if versioning is enabled),Can be used to delete incomplete Multi-Part uploads)
 - Amazon S3 Analytics – Storage Class Analysis
   - Help you decide when to transition objects to the right storage class
   - Recommendations for Standard and Standard IA (Does NOT work for One-Zone IA or Glacier)
   - Report is updated daily
   - 24 to 48 hours to start seeing data analysis
   - Good first step to put together Lifecycle Rules (or improve them)!
 - S3 – Requester Pays: In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket
   - With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket
   - Helpful when you want to share large datasets with other accounts
   - The requester must be authenticated in AWS (cannot be anonymous)
 - S3 Event Notifications: Can create as many “S3 events” as desired, Object name filtering possible, S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
   - SNS, SQS, Lambda (resources access policies)
   - S3 Event Notifications with Amazon EventBridge: 
     - Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
     - Advanced filtering options with JSON rules (metadata, object size, name...)
     - EventBridge Capabilities – Archive, Replay Events, Reliable delivery
 - S3 – Baseline Performance
   - Amazon S3 automatically scales to high request rates, latency 100-200 ms
   - Your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.
 - S3 Performance
   - Multi-Part upload: recommended for files > 100MB, must use for files > 5GB. Can help parallelize uploads (speed up transfers)
   - S3 Transfer Acceleration: Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region. Compatible with multi-part upload
   - S3 Performance – S3 Byte-Range Fetches: Parallelize GETs by requesting specific byte ranges. Better resilience in case of failures
 - S3 Select & Glacier Select: for simple filtering, Retrieve less data using SQL by performing server-side filtering using S3 select & glacier select. Can filter by rows & columns (simple SQL statements). Less network transfer, less CPU cost client-side
 - S3 Batch Operations: Perform bulk operations on existing S3 objects with a single request, example: Encrypt un-encrypted objects, Invoke Lambda function to perform custom action on each object, Restore objects from S3 Glacier...
   - A job consists of a list of objects, the action to perform, and optional parameters
   - S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports …
   - You can use **S3 Inventory** to get object list and use S3 Select to filter your objects
## S3 Security
 - Amazon S3 – Object Encryption: You can encrypt objects in S3 buckets using one of 4 methods
   - Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) – Enabled by Default
     - Encryption type is AES-256, owned by AWS
     - Enabled by default for new buckets & new objects
     - Must set header "x-amz-server-side-encryption": "AES256"
   - Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)
     - Encryption using keys handled and managed by AWS KMS (Key Management Service)
     - KMS advantages: user control + audit key usage using CloudTrail
     - Must set header "x-amz-server-side-encryption": "aws:kms"
     - KMS limits: request quotas
   - Server-Side Encryption with Customer-Provided Keys (SSE-C)
     - clients keep keys, Server-Side Encryption using keys fully managed by the customer outside of AWS
     - HTTPS must be used
     - Encryption key must provided in HTTP headers, for every HTTP request made
   - Client-Side Encryption
     - Use client libraries such as Amazon S3 Client-Side Encryption Library
     - Customer fully manages the keys and encryption cycle
     - clients must encrypt/decrypt data themselves
 - Amazon S3 – Encryption in transit (SSL/TLS): Encryption in flight is also called SSL/TLS
   - HTTP Endpoint – non encrypted
   - HTTPS Endpoint – encryption in flight (recommended, HTTPS is mandatory for SSE-C. Force encryption in flight by using resource access policy with condition: secureTransport)
 - Amazon S3 – Default Encryption vs. Bucket Policies
   - SSE-S3 encryption is automatically applied to new objects stored in S3 bucket
   - Optionally, you can “force encryption” using a bucket policy and refuse any API call to PUT an S3 object without encryption headers (SSE-KMS or SSE-C)
   - Note: Bucket Policies are evaluated before “Default Encryption”
 - What is CORS? (Cross-Origin Resource Sharing (CORS), Origin = scheme (protocol) + host (domain) + port)
   - Web Browser based mechanism to allow requests to other origins while visiting the main origin
   - The requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (example: Access-Control-Allow-Origin)
 - Amazon S3 – CORS: If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
 - Amazon S3 – MFA Delete: MFA (Multi-Factor Authentication) – force users to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3
   - MFA will be required to: Permanently delete an object version, Suspend Versioning on the bucket
   - MFA won’t be required to: Enable Versioning, List deleted versions
   - To use MFA Delete, Versioning must be enabled on the bucket
   - Only the bucket owner (root account) can enable/disable MFA Delete
 - S3 Access Logs
   - For audit purpose, you may want to log all access to S3 buckets
   - Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
   - That data can be analyzed using data analysis tools… (Athena)
   - The target logging bucket must be in the same AWS region
   - **warning:** Do not set your logging bucket to be the monitored bucket, It will create a logging loop, and your bucket will grow exponentially.
 - Amazon S3 – Pre-Signed URLs
   - Generate pre-signed URLs using the S3 Console, AWS CLI or SDK
   - Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET / PUT
   - URL Expiration:
     - S3 Console – 1 min up to 720 mins (12 hours)
     - AWS CLI – configure expiration with --expires-in parameter in seconds (default 3600 secs, max. 604800 secs ~ 168 hours)
   - examples: Allow temporarily a user to upload a file to a precise location in your S3 bucket; Allow only logged-in users to download a premium video from your S3
bucket...
 - S3 Glacier Vault Lock
   - Adopt a WORM (Write Once Read Many) model
   - Create a Vault Lock Policy
   - Lock the policy for future edits (can no longer be changed or deleted)
   - Helpful for compliance and data retention
 - S3 Object Lock (versioning must be enabled)
   - Adopt a WORM (Write Once Read Many) model
   - Block an object version deletion for a specified amount of time
   - Retention mode - Compliance: Object versions can't be overwritten or deleted by any user, including the root user. Objects retention modes can't be changed, and retention periods can't be shortened
   - Retention mode - Governance: Most users can't overwrite or delete an object version or alter its lock settings. Some users have special permissions to change the retention or delete the object.
   - Retention Period: protect the object for a fixed period, it can be extended
   - Legal Hold: protect the object indefinitely, independent from retention period. can be freely placed and removed using the s3:PutObjectLegalHold IAM permission
 - S3 – Access Points: Access Points simplify security management for S3 Buckets
   - Each Access Point has:
     - its own DNS name (Internet Origin or VPC Origin)
     - an access point policy (similar to bucket policy) – manage security at scale
 - S3 – Access Points –VPC Origin
   - We can define the access point to be accessible only from within the VPC
   - You must create a VPC Endpoint to access the Access Point (Gateway or Interface Endpoint)
   - The VPC Endpoint Policy must allow access to the target bucket and Access Point
 - S3 Object Lambda
   - Use AWS Lambda Functions to change the object before it is retrieved by the caller application
   - Only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Points.
   - Use Cases: Converting across data formats, such as converting XML to JSON. Redacting personally identifiable information for analytics or non-production environments.
## CloudFront and Global Accelerator
 - Amazon CloudFront:
   - Content Delivery Network (CDN) -- **public network**
   - Improves read performance, content is cached at the edge
   - more than 200 Point of Presence globally (edge locations)
   - DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall
 - CloudFront – Origins
   - S3 bucket:
     - For distributing files and caching them at the edge
     - along with S3 bucket policy, Enhanced security with CloudFront Origin Access Control (OAC) -- replacing Origin Access Identity (OAI) 
     - CloudFront can be used as an ingress (to upload files to S3)
   - Custom Origin (HTTP): ALB, EC2 instance, S3 website, any HTTP backend
 - CloudFront vs S3 Cross Region Replication
   - CloudFront: Global Edge network, Files are cached for a TTL (maybe a day), Great for static content that must be available everywhere
   - S3 Cross Region Replication: must setup manually, nearly real time, read-only, Great for **dynamic content** that needs to be available at low-latency in few regions
 - CloudFront Geo Restriction
   - You can restrict who can access your distribution: **Allowlist/Blocklist** : control users to access contents based on the countries they are in.
   - the "country" is determined by 3rd party Geo-IP DB
   - Use case: Copyright Laws to control access to content
 - CloudFront - Pricing
   - CloudFront Edge locations are all around the world
   - The cost of data out per edge location varies
 - CloudFront – Price Classes
   - You can reduce the number of edge locations for cost reduction
   - Three price classes:
     - Price Class All: all regions – best performance
     - Price Class 200: most regions, but excludes the most expensive regions
     - Price Class 100: only the least expensive regions
 - CloudFront – Cache Invalidations
   - In case you update the back-end origin, CloudFront doesn’t know about it and will only get the refreshed content after the TTL has expired
   - However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a CloudFront Invalidation
   - You can invalidate all files (*) or a special path (/images/*)
 - Global users for our application
   - You have deployed an application and have global users who want to access it directly.
   - They go over the public internet, which can add a lot of latency due to many hops
   - We wish to go as fast as possible through AWS network to minimize latency
 - Unicast IP vs Anycast IP
   - Unicast IP: one server holds one IP address
   - Anycast IP: all servers hold the same IP address and the client is routed to the nearest one
 - AWS Global Accelerator
   - Leverage the AWS **internal** network to route to your application
   - 2 Anycast IP are created for your application
   - The Anycast IP send traffic directly to Edge Locations, The Edge locations send the traffic to your application
   - Works with Elastic IP, EC2 instances, ALB, NLB, public or private
   - Consistent Performance: Intelligent routing to lowest latency and fast regional failover, No issue with client cache (because the IP doesn’t change), Internal AWS network
   - Health Checks: Global Accelerator performs a health check of your applications, Helps make your application global (failover less than 1 minute for unhealthy), Great for disaster recovery (thanks to the health checks)
   - Security: only 2 external IP need to be whitelisted, DDoS protection thanks to AWS Shield
 - AWS Global Accelerator vs CloudFront :  both use the AWS global network and its edge locations around the world. Both services integrate with AWS Shield for DDoS protection.
   - CloudFront
     - Improves performance for both cacheable content (such as images and videos)
     - Dynamic content (such as API acceleration and dynamic site delivery)
     - Content is served at the edge
   - Global Accelerator (no caching)
     - Improves performance for a wide range of applications over TCP or UDP
     - Proxying packets at the edge to applications running in one or more AWS Regions.
     - Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP
     - Good for HTTP use cases that require static IP addresses
     - Good for HTTP use cases that required deterministic, fast regional failover
## AWS Storage Extras
 - AWS Snow Family: Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS
   - Data migration: Snowcone, Snowball edge, Snowmobile
   - Edge computing: Snowcone, Snowball edge
 - Data Migrations with AWS Snow Family
   - AWS Snow Family: offline devices to perform data migrations. If it takes more than a week to transfer over the network, use Snowball devices!
 - Snowball Edge (for data transfers): Physical data transport solution: move TBs or PBs of data in or out of AWS
   - Pay per data transfer job
   - Provide block storage and Amazon S3-compatible object storage
   - Snowball Edge Storage Optimized: 80TB of HDD for block volume and S3 compatible object storage
   - Snowball Edge Compute Optimized: 42 TB of HDD or 28TB NVMe capacity for block volume and S3 compatible object storage
 - AWS Snowcone & Snowcone SSD: Small, portable computing, anywhere, rugged & secure, withstands harsh environments
   - Snowcone– 8 TB of HDD Storage
   - Snowcone SSD – 14 TB of SSD Storage
   - Use Snowcone where Snowball does not fit (space-constrained environment)
   - Must provide your own battery / cables
   - Can be sent back to AWS offline, or connect it to internet and use AWS **DataSync** to send data
 - AWS Snowmobile
   - Transfer exabytes of data (1 EB = 1,000 PB = 1,000,000 TBs)
   - Each Snowmobile has 100 PB of capacity (use multiple in parallel)
   - Better than Snowball if you transfer more than 10 PB
   - High security: temperature controlled, GPS, 24/7 video surveillance
 - Snow Family – Usage Process
   - Request Snowball devices from the AWS console for delivery
   - Install the snowball client / AWS OpsHub on your servers
   - Connect the snowball to your servers and copy files using the client
   - Ship back the device when you’re done (goes to the right AWS facility)
   - Data will be loaded into an S3 bucket
   - Snowball is completely wiped
 - What is Edge Computing?
   - Process data while it’s being created on an edge location (Limited / no internet access, Limited / no easy access to computing power)
   - We setup a Snowball Edge / Snowcone device to do edge computing
   - Use cases of Edge Computing: Preprocess data, Machine learning at the edge, Transcoding media streams
   - Eventually (if need be) we can ship back the device to AWS (for transferring data for example)
 - Snow Family – Edge Computing
   - Snowcone & Snowcone SSD (smaller): 2 CPUs, 4 GB of memory, wired or wireless access. USB-C power using a cord or the optional battery
   - Snowball Edge – Compute Optimized: 104 vCPUs, 416 GiB of RAM, Optional GPU (useful for video processing or machine learning), 28TB NVMe or 42TB HDD usable storage, Storage Clustering available (up to 16 nodes)
   - Snowball Edge – Storage Optimized: Up to 40 vCPUs, 80 GiB of RAM, 80 TB storage
   - All: Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)
   - Long-term deployment options: 1 and 3 years discounted pricing
 - AWS OpsHub: Historically, to use Snow Family devices, you needed a CLI (Command Line Interface tool), Today, you can use AWS OpsHub (a software you install on your computer / laptop) to manage your Snow Family Device: configuration, management, monitoring, running compatible AWS services.
 - Solution Architecture: Snowball into Glacier
   - **Snowball cannot import to Glacier directly.** You must use Amazon S3 first, in combination with an S3 lifecycle policy
 - Amazon FSx – Overview: Fully managed service, Launch 3rd party high-performance file systems on AWS
   - FSx for Lustre(Linux & cluster): Lustre is a type of parallel distributed file system, for large-scale computing
     - Machine Learning, `High Performance Computing (HPC)`
     - `Seamless integration with S3` (read and write through FSx)
     - storage options: HDD & SDD
     - Video Processing, Financial Modeling, Electronic Design Automation
     - `Can be used from on-premises servers (VPN or Direct Connect)`
   - FSx for NetApp ONTAP: Managed NetApp ONTAP on AWS.
     - `File System compatible with NFS, SMB, iSCSI protocol`
     - `Point-in-time instantaneous cloning (helpful for testing new workloads)`
     - Snapshots, replication, low-cost, compression and data de-duplication
     - Storage shrinks or grows automatically
     - Works with: linux, windows, macos, EC2, ECS, EKS, & AppStream 2.0 ...
     - Move workloads running on ONTAP or NAS to AWS
   - FSx for Windows File Server: FSx for Windows is a fully managed Windows file system share drive
     - Supports SMB protocol & Windows NTFS
     - Microsoft Active Directory integration, ACLs, user quotas
     - `Can be mounted on Linux EC2 instances`
     - Supports `Microsoft's Distributed File System (DFS) Namespaces` (group files across multiple FS)
     - Can be accessed from your on-premises infrastructure (VPN or Direct Connect)
     - Can be configured to be Multi-AZ (high availability)
     - Data is backed-up daily to S3
     - Storage Options: SDD & HDD
   - FSx for OpenZFS: Managed OpenZFS file system on AWS
     - `Point-in-time instantaneous cloning (helpful for testing new workloads)`
     - File System compatible with NFS (v3, v4, v4.1, v4.2)
     - Snapshots, compression and low-cost
     - Move workloads running on ZFS to AWS
     - Works with: linux, windows, macos, EC2, ECS, EKS, & AppStream 2.0 ...
 - FSx Lustre - File System Deployment Options
   - Scratch File System: Temporary storage, Data is not replicated (doesn’t persist if file server fails), High burst. Usage: short-term processing, optimize costs
   - Persistent File System: Long-term storage, Data is replicated within same AZ. Replace failed files within minutes. Usage: long-term processing, sensitive data
 - Hybrid Cloud for Storage
   - AWS is pushing for ”hybrid cloud”: on-premises & Cloud
   - due to: long migration, security requirements, compliance requirements, IT strategy
   - S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premises? `AWS Storage Gateway!`
 - AWS Storage Cloud Native Options
   - Block: EBS, EC2 instance store
   - File: AWS EFS, FSx
   - Object: AWS S3, AWS Glacier
 - AWS Storage Gateway: Bridge between on-premises data and cloud data
   - use cases: disaster recovery/ backup & restore / on-premises cache & low-latency files access / tiered storage
   - types:
     - S3 File Gateway
       - Configured S3 buckets are accessible using the NFS and SMB protocol
       - `Most recently used data is cached in the file gateway`
       - support S3 storage classes
       - `Transition to S3 Glacier using a Lifecycle Policy`
       - SMB Protocol has integration with Active Directory (AD) for user authentication
       - Bucket access using IAM roles for each File Gateway
     - FSx File Gateway
       - Native access to Amazon FSx for Windows File Server
       - `Local cache for frequently accessed data`
       - Windows native compatibility (SMB, NTFS, Active Directory...)
       - Useful for group file shares and home directories
     - Volume Gateway
       - Block storage using `iSCSI` protocol backed by S3
       - Backed by EBS snapshots which can help restore on-premises volumes!
       - Cached volumes: low latency access to most recent data
       - Stored volumes: entire dataset is on premise, scheduled backups to S3
     - Tape Gateway
       - Some companies have backup processes using physical tapes (!)
       - With Tape Gateway, companies use the same processes but, in the cloud
       - Virtual Tape Library (VTL) backed by Amazon S3 and Glacier
       - Back up data using existing tape-based processes (and iSCSI interface)
       - Works with leading backup software vendors
 - Storage Gateway – Hardware appliance
   - Using Storage Gateway means you need on-premises virtualization
   - `Otherwise, you can use a Storage Gateway Hardware Appliance`
   - Works with File Gateway, Volume Gateway, Tape Gateway
   - Helpful for daily NFS backups in small data centers
 - AWS Transfer Family: A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol
   - support protocols: `AWS Transfer for FTP, AWS Transfer for FTPS, AWS Transfer for SFTP`
   - Store and manage users’ credentials within the service
   - Integrate with existing authentication systems: microsoft AD, Okta, AWS Cognito, ...
   - use cases: sharing files, CRM, ERP...
 - AWS DataSync
   - Move large amount of data to and from: on-prem / other cloud to AWS, needs agent ; aws-aws no need agent
   - Can synchronize to: S3, EFS, FSx
   - Replication tasks can be scheduled hourly, daily, weekly
   - `File permissions and metadata are preserved (NFS POSIX, SMB…)`
   - One agent task can use 10 Gbps, can setup a bandwidth limit
 - Storage comparison:
   - S3: Object Storage
   - S3 Glacier: Object Archival
   - EBS volumes: Network storage for one EC2 instance at a time
   - Instance Storage: Physical storage for your EC2 instance (high IOPS)
   - EFS: Network File System for Linux instances, POSIX filesystem
   - FSx for Windows: Network File System for Windows servers
   - FSx for Lustre: High Performance Computing Linux file system
   - FSx for NetApp ONTAP: High OS Compatibility
   - FSx for OpenZFS: Managed ZFS file system
   - Storage Gateway: S3 & FSx File Gateway, Volume Gateway (cache & stored), Tape Gateway
   - Transfer Family: FTP, FTPS, SFTP interface on top of Amazon S3 or Amazon EFS
   - DataSync: Schedule data sync from on-premises to AWS, or AWS to AWS
   - Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically
   - Database: for specific workloads, usually with indexing and querying
## AWS Integration and Messaging
 - Amazon SQS (fully-managed)
   - Amazon SQS – Standard Queue (Low latency)
     - Can have duplicate messages (at least once delivery, occasionally)
     - Can have out of order messages (best effort ordering)
     - Unlimited throughput, unlimited number of messages in queue
     - Default retention of messages: 4 days, maximum of 14 days
     - Limitation of 256KB per message sent
   - SQS – Producing Messages: Produced to SQS using the SDK (SendMessage API)
     - The message is `persisted` in SQS until a consumer deletes it
     - Message retention: default 4 days, up to 14 days
   - SQS – Consuming Messages: Consumers (running on EC2 instances, servers, or AWS Lambda)…
     - Poll SQS for messages (receive up to 10 messages at a time)
     - Delete the messages using the DeleteMessage API (similar to `ack` in RabbitMQ)
   - SQS – Multiple EC2 Instances Consumers
     - Consumers receive and process messages `in parallel`
     - At least once delivery
     - Consumers delete messages after processing them
     - We can scale consumers horizontally to improve throughput of processing
   - SQS with Auto Scaling Group (ASG):  using CloudWatch to monitor the metric for SQS, then trigger a cloudWatch alarm to ask ASG to scale up/down the group.
   - SQS to decouple between application tiers
   - Amazon SQS - Security
     - Encryption:
       - In-flight encryption using HTTPS API
       - At-rest encryption using KMS keys
       - Client-side encryption if the client wants to perform encryption/decryption itself
     - Access Controls: IAM policies to regulate access to the SQS API
     - SQS Access Policies (similar to S3 bucket policies)
 - SQS – Message Visibility Timeout
   - After a message is polled by a consumer, it becomes `invisible` to other consumers
   - By default, the “message visibility timeout” is 30 seconds, That means the message has 30 seconds to be processed
   - After the message visibility timeout is over, the message is “visible” in SQS
   - If a message is not processed within the visibility timeout, it will be processed `twice`
   - `A consumer could call the **ChangeMessageVisibility** API to get more time`
   - If visibility timeout is high (hours), and consumer crashes, re-processing will take time
   - If visibility timeout is too low (seconds), we may get duplicates
 - Amazon SQS - Long Polling
   - When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue
   - **LongPolling decreases the number of API calls made to SQS while increasing the efficiency and reducing latency of your application**
   - The wait time can be between 1 sec to 20 sec(20 sec preferable)
   - Long Polling is preferable to Short Polling
   - Long polling can be enabled at the queue level or at the API level using **WaitTimeSeconds**
 - Amazon SQS – FIFO Queue
   - Limited throughput: 300 msg/s without batching, 3000 msg/s with
   - Exactly-once send capability (by removing duplicates)
   - Messages are processed in order by the consumer
 - Amazon SNS
   - The “event producer” only sends message to one SNS topic
   - Each subscriber to the topic will get all the messages (note: new feature to filter messages)
   - Up to 12,500,000 subscriptions per topic
   - 100,000 topics limit
 - SNS integrates with a lot of AWS services(event producers)
 - Amazon SNS – How to publish
   - Topic Publish (using the SDK): topic, then subscription, then publish
   - Direct Publish (for mobile apps SDK)
     - Create a platform application
     - Create a platform endpoint
     - Publish to the platform endpoint
     - Works with Google GCM, Apple APNS, Amazon ADM…
 - Amazon SNS – Security  (similar to SQS)
   - Encryption:
     - in-flight via HTTPs
     - at rest via KMS key
     - client-side encryption
   - Access Controls: IAM policies to regulate access to the SNS API
   - SNS Access Policies (similar to S3 bucket policies)
 - SNS + SQS: Fan Out
   - push to SNS, then send to multiple SQS queue subscribers which allows retention, delay processing and retries
   - Make sure your SQS queue `access policy` allows for SNS to write
   - Cross-Region Delivery: works with SQS Queues in other regions
   - Application: S3 Events to multiple queues
   - Application: SNS to Amazon S3 through Kinesis Data Firehose(have multiple destinations, including S3)
 - Amazon SNS – FIFO Topic (similar to SQS FIFO)
   - Similar features as SQS FIFO:
     - Ordering by Message Group ID (all messages in the same group are ordered)
     - Deduplication using a Deduplication ID or Content Based Deduplication
   - Can have SQS Standard and FIFO queues as subscribers
 - SNS FIFO + SQS FIFO: Fan Out (In case you need fan out + ordering + deduplication)
 - SNS – Message Filtering
   - JSON policy used to filter messages sent to SNS topic’s subscriptions
   - If a subscription doesn’t have a filter policy, it receives every message
 - Kinesis Overview
   - Makes it easy to collect, process, and analyze streaming data in real-time
   - Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data…
   - components:
     - Kinesis Data Streams: capture, process, and store data streams
       - Retention between 1 day to 365 days
       - able to replay data
       - Once data is inserted in Kinesis, it can’t be deleted (immutability)
       - Data that shares the same partition goes to the same shard (ordering)
       - Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
       - Consumers: Kinesis Client Library (KCL), AWS SDK,  AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics,
       - Capacity Modes:
         - Provisioned mode: You choose the number of shards provisioned, scale manually or using API. Each shard gets 1MB/s in. Each shard gets 2MB/s out. You pay per shard provisioned per hour.
         - On-demand mode: No need to provision or manage the capacity. Default capacity provisioned (4 MB/s in or 4000 records per second). Scales automatically based on observed throughput peak during the last 30 days. Pay per stream per hour & data in/out per GB.
       - Security:
         - Control access / authorization using IAM policies
         - Encryption: in-flight via HTTPS, at rest via KMS key, or client-side encryption
         - VPC Endpoints available for Kinesis to access within VPC
         - Monitor API calls using CloudTrail
     - Kinesis Data Firehose: load data streams into AWS data stores
       - Fully Managed Service, no administration, automatic scaling, serverless
       - destinations:
         - AWS: Redshift / Amazon S3 / OpenSearch
         - 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / …
         - Custom: send to any HTTP endpoint
       - Pay for data going through Firehose
       - `Near Real Time`: 60 seconds latency minimum for non full batches, Or minimum 1 MB of data at a time
       - Supports many data formats, conversions, transformations(custom transform by lambda), compression
       - Can send failed or all data to a backup S3 bucket
       - compare with Kinesis Data Streams:
         - Kinesis Data Streams: Streaming service for ingest at scale. Real-time. retention 1-365 days. support replay. Manage scaling (shard splitting /merging)
         - Kinesis Data Firehose: Load streaming data into S3 / Redshift /OpenSearch / 3rd party / custom HTTP. Near real-time. auto scaling. No data storage. no replay.
     - Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
     - Kinesis Video Streams: capture, process, and store video streams
   - Ordering data into Kinesis
     - **The same partition key will always go to the same shard, thus it guarantees the order**
   - Ordering data into SQS
     - For SQS standard, there is no ordering.
     - For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with `only one consumer`
     - You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other, Then you use a Group ID (similar to Partition Key in Kinesis)
   - Kinesis vs SQS ordering
     - Kinesis data stream: shards guarantee the order, each shard can receive 1MB/s (high throughput), using partition key to have multiple consumers
     - SQS: FIFO, can have multiple consumers by using group ID
 - Amazon MQ
   - Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS
   - When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ : RabbitMQ, avtiveMQ
   - Amazon MQ runs on servers, can run in Multi-AZ with failover
   - doesn't scale as much as SQS, SNS
## Containers on AWS
 - What is Docker? Docker is a software development platform to deploy apps. Apps are packaged in containers that can be run on any OS. Use cases: microservices architecture, lift-and-shift apps from on-premises to the AWS cloud, …
 - Where are Docker images stored? Docker images are stored in Docker Repositories. Docker Hub / Amazon ECR (Amazon Elastic Container Registry): public/private((Amazon ECR Public Gallery)
 - Docker vs. Virtual Machines
   - Docker is ”sort of” a virtualization technology, but not exactly
 - Docker Containers Management on AWS
   - Amazon Elastic Container Service (Amazon ECS): Amazon’s own container platform
   - Amazon Elastic Kubernetes Service (Amazon EKS): Amazon’s managed Kubernetes (open source)
   - AWS Fargate: Amazon’s own Serverless container platform. Works with ECS and with EKS
   - Amazon ECR: Store container images
 - Amazon ECS - EC2 Launch Type
   - Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters
   - `EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)`
   - Each EC2 Instance must run the ECS Agent to register in the ECS Cluster
   - AWS takes care of starting / stopping containers
 - Amazon ECS – Fargate Launch Type
   - Launch Docker containers on AWS
   - `You do not provision the infrastructure (no EC2 instances to manage)`
   - `It’s all Serverless!`
   - You just create task definitions
   - AWS just runs ECS Tasks for you based on the CPU / RAM you need
   - To scale, just increase the number of tasks. Simple - no more EC2 instances
 - Amazon ECS – IAM Roles for ECS
   - EC2 Instance Profile (EC2 Launch Type only):
     - Used by the ECS agent
     - Makes API calls to ECS service
     - Send container logs to CloudWatch Logs
     - Pull Docker image from ECR
     - Reference sensitive data in Secrets Manager or SSM Parameter Store
   - ECS Task Role:
     - Allows each task to have a specific role
     - Use different roles for the different ECS Services you run
     - Task Role is defined in the task definition
 - Amazon ECS – Load Balancer Integrations
   - Application Load Balancer supported and works for most use cases
   - Network Load Balancer recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link
   - Classic Load Balancer supported but not recommended (no advanced features – no Fargate)
 - Amazon ECS – Data Volumes (EFS)
   - Mount EFS file systems onto ECS tasks
   - Works for both EC2 and Fargate launch types
   - Tasks running in any AZ will share the same data in the EFS file system
   - Fargate + EFS = Serverless
   - Use cases: persistent multi-AZ shared storage for your containers
   - Note: Amazon S3 cannot be mounted as a file system
 - ECS Service Auto Scaling
   - Automatically increase/decrease the desired number of ECS tasks
   - Amazon ECS Auto Scaling uses AWS `Application Auto Scaling`
     - ECS Service Average CPU Utilization
     - ECS Service Average Memory Utilization - Scale on RAM
     - ALB Request Count Per Target – metric coming from the ALB
   - Target Tracking – scale based on target value for a specific CloudWatch metric
   - Step Scaling – scale based on a specified CloudWatch Alarm
   - Scheduled Scaling – scale based on a specified date/time (predictable changes)
   - ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)
   - Fargate Auto Scaling is much easier to setup (because Serverless)
 - EC2 Launch Type – Auto Scaling EC2 Instances
   - Accommodate ECS Service Scaling by adding underlying EC2 Instances
   - `Auto Scaling Group Scaling:`
     - Scale your ASG based on CPU Utilization
     - Add EC2 instances over time
   - `ECS Cluster Capacity Provider` (recommended)
     - Used to automatically provision and scale the infrastructure for your ECS Tasks
     - **Capacity Provider paired with an Auto Scaling Group**
     - Add EC2 Instances when you’re missing capacity (CPU, RAM…)
 - Amazon ECR: Elastic Container Registry
   - Store and manage Docker images on AWS
   - Private and Public repository (Amazon ECR Public Gallery)
   - Fully integrated with ECS, backed by Amazon S3
   - Access is controlled through IAM (permission errors => policy)
   - Supports image vulnerability scanning, versioning, image tags, image lifecycle, …
 - Amazon EKS Overview: Amazon Elastic Kubernetes Service
   - Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application
   - It’s an alternative to ECS, similar goal but different API
   - EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers
   - Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes
   - Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP…)
   - For multiple regions, deploy one EKS cluster per region
   - Collect logs and metrics using `CloudWatch Container Insights`
 - Amazon EKS – Node Types
   - Managed Node Groups
     - Creates and manages Nodes (EC2 instances) for you
     - Nodes are part of an ASG managed by EKS
     - Supports On-Demand or Spot Instances
   - Self-Managed Nodes:
     - Nodes created by you and registered to the EKS cluster and managed by an ASG
     - You can use prebuilt AMI - Amazon EKS Optimized AMI
     - Supports On-Demand or Spot Instances
   - AWS Fargate: No maintenance required; no nodes managed
 - Amazon EKS – Data Volumes
   - Need to specify `StorageClass` manifest on your EKS cluster
   - Leverages a Container Storage Interface (CSI) compliant driver
   - support for: EBS, EFS(works for fargate), Amazon FSx for Lustre, Amazon FSx for NetApp ONTAP
 - AWS App Runner
   - Fully managed service that makes it easy to deploy web applications and APIs at scale
   - No infrastructure experience required
   - Start with your source code or container image
   - Automatically builds and deploy the web app
   - Automatic scaling, highly available, load balancer, encryption
   - VPC access support
   - Connect to database, cache, and message queue services
   - Use cases: web apps, APIs, microservices, rapid production deployments
## Serverless Overview
 - What’s serverless? Serverless does not mean there are no servers... it means you just don’t manage / provision / see them. Serverless was pioneered by AWS Lambda but now also includes anything that’s managed: “databases, messaging, storage, etc.”
 - Serverless in AWS: AWS Lambda, DynamoDB, AWS Cognito, AWS API Gateway, Amazon S3, AWS SNS & SQS, AWS Kinesis Data Firehose, Aurora Serverless, Step Functions, Fargate
 - Lambda:
   - Virtual functions – no servers to manage!
   - Limits:
     - short executions (limit: 15 min)
     - Environment variables (4 KB)
     - Memory allocation: 128 MB – 10GB (1 MB increments)
     - Disk capacity in the “function container” (in /tmp): 512 MB to 10GB
     - Concurrency executions: 1000 (can be increased)
     - Deployment:
       - Lambda function deployment size (compressed .zip): 50 MB
       - Size of uncompressed deployment (code + dependencies): 250 MB
       - Can use the /tmp directory to load other files at startup
   - Run on-demand
   - Scaling is automated!
   - easy pricing: first 1 million calls are free, 400,000GB-seconds per month are free
   - support multiple programming languages
   - Integrated with the whole AWS suite of services
   - Easy monitoring through AWS CloudWatch
   - Increasing RAM will also improve CPU and network!
   - snapstart: only support Java 11 or above. When enabled, the function is invoked from pre-initialized state. when publishes a new version, a snapshot of memory and disk will be taken.
 - Customization At The Edge: Many modern applications execute some form of the logic at the edge
   - Edge Function: A code that you write and attach to CloudFront distributions. Runs close to your users to minimize latency
   - CloudFront provides two types: CloudFront Functions & Lambda@Edge
   - You don’t have to manage any servers, deployed globally
   - serverless
   - Use case: customize the CDN content
 - CloudFront Functions & Lambda@Edge Use Cases:
   - Website Security and Privacy
   - Bot Mitigation at the Edge
   - Real-time Image Transformation
   - A/BTesting
   - User Authentication and Authorization
   - Search Engine Optimization (SEO)
   - User Tracking and Analytics
 - CloudFront Functions
   - Lightweight functions written in JavaScript
   - For high-scale, latency-sensitive CDN customizations
   - native feature, used to modify the viewer request and response
   - Sub-ms startup times, millions of requests/second
 - Lambda@Edge (more powerful, no free tier)
   - Lambda functions written in NodeJS or Python
   - Scales to 1000s of requests/second
   - Used to change CloudFront requests and responses (viewer and origin)
   - Author your functions in one AWS Region (us-east-1), then CloudFront replicates to its locations
 - CloudFront Functions vs. Lambda@Edge - Use Cases
   - CloudFront Functions: Cache key normalization. Header manipulation. URL rewrites or redirects. Request authentication & authorization.
   - Lambda@Edge: Longer execution time (several ms). Adjustable CPU or memory. Your code depends on a 3rd libraries (e.g., AWS SDK to access other AWS services). Network access to use externaly services for processing. File system access or access to the body of HTTP requests.
 - Lambda by default
   - By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)
   - Therefore, it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)
 - Lambda in VPC
   - You must define the VPC ID, the Subnets and the Security Groups
   - Lambda will create an ENI (Elastic Network Interface) in your subnets
 - Lambda with RDS Proxy
   - If Lambda functions directly access your database, they may open too many connections under high load
   - RDS Proxy:
     - Improve scalability by pooling and sharing DB connections
     - Improve availability by reducing by 66% the failover time and preserving connections
     - Improve security by enforcing IAM authentication and storing credentials in Secrets Manager
   - **The Lambda function must be deployed in your VPC, because RDS Proxy is never publicly accessible**
 - Invoking Lambda from RDS & Aurora
   - Invoke Lambda functions from within your DB instance
   - Allows you to process data events from within a database
   - Supported for RDS for PostgreSQL and Aurora MySQL
   - Must allow outbound traffic to your Lambda function from within your DB instance (Public, NAT GW,VPC Endpoints)
   - DB instance must have the required permissions to invoke the Lambda function (Lambda Resource-based Policy & IAM Policy)
 - RDS Event Notifications
   - Notifications that tells information about the DB instance itself (created, stopped, start, ...)
   - You don’t have any information about the data itself
   - Subscribe to the following event categories: DB instance, DB snapshot, DB Parameter Group, DB Security Group, RDS Proxy, Custom Engine Version
   - Near real-time events (up to 5 minutes)
   - Send notifications to SNS or subscribe to events using EventBridge
 - Amazon DynamoDB
   - Fully managed, highly available with replication across multiple AZs
   - NoSQL database - not a relational database - with transaction support
   - Scales to massive workloads, distributed database
   - Millions of requests per seconds, trillions of row, 100s of TB of storage
   - Fast and consistent in performance (single-digit millisecond)
   - Integrated with IAM for security, authorization and administration
   - Low cost and auto-scaling capabilities
   - No maintenance or patching, always available
   - Standard & Infrequent Access (IA) Table Class
   - Basics:
     - DynamoDB is made of Tables
     - Each table has a Primary Key (must be decided at creation time)
     - Each table can have an infinite number of items (= rows)
     - Each item has attributes (can be added over time – can be null)
     - Maximum size of an item is 400KB
     - Data types supported are: Scalar Types(primitives), Document Types – List, Map, Set Types – String Set, Number Set, Binary Set
     - Therefore, in DynamoDB you can rapidly evolve schemas
   - Read/Write Capacity Modes
     - Control how you manage your table’s capacity (read/write throughput)
     - Provisioned Mode (default): You specify the number of reads/writes per second. You need to plan capacity beforehand. Pay for provisioned Read Capacity Units (RCU) & Write Capacity Units (WCU). **Possibility to add auto-scaling mode for RCU & WCU**
     - On-Demand Mode: Read/writes automatically scale up/down with your workloads. No capacity planning needed. Pay for what you use, more expensive ($$$). Great for unpredictable workloads, steep sudden spikes.
   - DynamoDB Accelerator (DAX)
     - Fully-managed, highly available, seamless in- memory cache for DynamoDB
     - Help solve read congestion by caching
     - Microseconds latency for cached data
     - Doesn’t require application logic modification (compatible with existing DynamoDB APIs)
     - 5 minutes TTL for cache (default)
     - DynamoDB Accelerator (DAX) vs. ElastiCache
       - Amazon ElastiCache: Store Aggregation Result
       - DynamoDB Accelerator (DAX): - Individual objects cache - Query & Scan cache
   - DynamoDB – Stream Processing
     - Ordered stream of item-level modifications (create/update/delete) in a table
     - Use cases: Real-time usage analytics. React to changes in real-time (welcome email to users). Invoke AWS Lambda on changes to your DynamoDB table. Implement cross-region replication. Insert into derivative tables
     - 24 hrs retention, limited consumers Process using AWS Lambda Triggers, or DynamoDB Stream Kinesis adapter
     - vs Kinesis Data Streams (newer):
       - 1 yr retention, high number of consumers, Process using AWS Lambda, Kinesis Data Analytics, Kineis Data Firehose, AWS Glue Streaming ETL...
   - DynamoDB Global Tables
     - Make a DynamoDB table accessible with low latency in multiple-regions
     - Active-Active replication (two-way replications)
     - Applications can READ and WRITE to the table in any region
     - `Must enable DynamoDB Streams as a pre-requisite`
   - DynamoDB –TimeTo Live (TTL)
     - Automatically delete items after an expiry timestamp
     - Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, web session handling...
   - Backups for disaster recovery
     - Continuous backups using point-in-time recovery (PITR)
       - Optionally enabled for the last 35 days
       - Point-in-time recovery to any time within the backup window
       - The recovery process creates a new table
     - On-demand backups:
       - Full backups for long-term retention, until explicitely deleted
       - Doesn’t affect performance or latency
       - Can be configured and managed in AWS Backup (enables cross-region copy)
       - The recovery process creates a new table
   - Integration with Amazon S3
     - Export to S3 (must enable PITR)
       - Works for any point of time in the last 35 days
       - Doesn’t affect the read capacity of your table
       - Perform data analysis on top of DynamoDB
       - Retain snapshots for auditing
       - ETL on top of S3 data before importing back into DynamoDB
       - Export in DynamoDB JSON or ION format
     - Import from S3
       - Import CSV, DynamoDB JSON or ION format
       - Doesn’t consume any write capacity
       - Creates a new table
       - Import errors are logged in CloudWatch Logs
 - AWS API Gateway
   - AWS Lambda + API Gateway: No infrastructure to manage
   - Support for the WebSocket Protocol
   - Handle API versioning (v1, v2...)
   - Handle different environments (dev, test, prod...)
   - Handle security (Authentication and Authorization)
   - Create API keys, handle request throttling
   - Swagger / Open API import to quickly define APIs
   - Transform and validate requests and responses
   - Generate SDK and API specifications
   - Cache API responses
 - API Gateway – Integrations High Level
   - Lambda Function: Easy way to expose REST API backed by AWS Lambda
   - HTTP: Expose HTTP endpoints in the backend. Example: internal HTTP API on premise, Application Load Balancer... Why? Add rate limiting, caching, user authentications, API keys, etc...
   - AWS Service:Expose any AWS API through the API Gateway. Example: start an AWS Step Function workflow, post a message to SQS. Why? Add authentication, deploy publicly, rate control...
 - API Gateway - Endpoint Types
   - Edge-Optimized (default): For global clients Requests are routed through the CloudFront Edge locations (improves latency), The API Gateway still lives in only one region.
   - Regional: For clients within the same region, Could manually combine with CloudFront (more control over the caching strategies and the distribution)
   - Private: Can only be accessed from your VPC using an interface VPC endpoint (ENI). Use a resource policy to define access
 - API Gateway – Security
   - User Authentication through
     - IAM Roles (useful for internal applications)
     - Cognito (identity for external users – example mobile users)
     - Custom Authorizer (your own logic)
   - Custom Domain Name HTTPS security through integration with AWS Certificate Manager (ACM)
     - If using Edge-Optimized endpoint, then the certificate must be in us-east-1
     - If using Regional endpoint, the certificate must be in the API Gateway region
     - Must setup CNAME or A-alias record in Route 53
 - AWS Step Functions
   - Build serverless visual workflow to orchestrate your Lambda functions
   - Features: sequence, parallel, conditions, timeouts, error handling, ...
   - Can integrate with EC2, ECS, On-premises servers, API Gateway, SQS queues, etc...
   - Possibility of implementing human approval feature
   - Use cases: order fulfillment, data processing, web applications, any workflow
 - Amazon Cognito
   - Give users an identity to interact with our web or mobile application
   - Cognito User Pools:Sign in functionality for app users, Integrate with API Gateway & Application Load Balancer
     - Create a serverless database of user for your web & mobile apps
     - Simple login
     - Password reset
     - Email & Phone Number Verification
     - Multi-factor authentication (MFA)
     - Federated Identities: users from Facebook, Google, SAML...
   - Cognito Identity Pools (Federated Identity):Provide AWS credentials to users so they can access AWS resources directly, Integrate with Cognito User Pools as an identity provider
     - Get identities for “users” so they obtain temporary AWS credentials
     - Users source can be Cognito User Pools, 3rd party logins, etc...
     - Users can then access AWS services directly or through API Gateway
     - The IAM policies applied to the credentials are defined in Cognito
     - They can be customized based on the user_id for fine grained control
     - Default IAM roles for authenticated and guest users
   - Cognito vs IAM: “hundreds of users”, ”mobile users”, “authenticate with SAML”
## Serverless Architecture
 - Mobile application: MyTodoList
   - Serverless REST API: HTTPS, API Gateway, Lambda, DynamoDB
   - Using Cognito to generate temporary credentials with STS to access S3 bucket with restricted policy. App users can directly access AWS resources this way. Pattern can be applied to DynamoDB, Lambda...
   - Caching the reads on DynamoDB using DAX
   - Caching the REST requests at the API Gateway level
   - Security for authentication and authorization with Cognito, STS
 - Serverless hosted website: MyBlog.com
   - We’ve seen static content being distributed using CloudFront with S3
   - The REST API was serverless, didn’t need Cognito because public
   - We leveraged a Global DynamoDB table to serve the data globally
   - (we could have used Aurora Global Database)
   - We enabled DynamoDB streams to trigger a Lambda function
   - The lambda function had an IAM role which could use SES
   - SES (Simple Email Service) was used to send emails in a serverless way
   - S3 can trigger SQS / SNS / Lambda to notify of events
 - Micro Services architecture
   - You are free to design each micro-service the way you want • Synchronous patterns: API Gateway, Load Balancers
   - Asynchronous patterns: SQS, Kinesis, SNS, Lambda triggers (S3)
   - Challenges with micro-services:
     - repeated overhead for creating each new microservice,
     - issues with optimizing server density/utilization
     - complexity of running multiple versions of multiple microservices simultaneously
     - proliferation of client-side code requirements to integrate with many separate services.
   - Some of the challenges are solved by Serverless patterns:
     - API Gateway, Lambda scale automatically and you pay per usage
     - You can easily clone API, reproduce environments
     - Generated client SDK through Swagger integration for the API Gateway
 - Software updates offloading
   - Why CloudFront?
     - No changes to architecture
     - Will cache software update files at the edge
     - Software update files are not dynamic, they’re static (never changing) • Our EC2 instances aren’t serverless
     - But CloudFront is, and will scale for us
     - Our ASG will not scale as much, and we’ll save tremendously in EC2 • We’ll also save in availability, network bandwidth cost, etc
     - Easy way to make an existing application more scalable and cheaper!
## Databases in AWS
 - RDBMS (= SQL / OLTP): RDS, Aurora – great for joins
   - RDS: managed PostgreSQL / MySQL / Oracle / SQL Server / MariaDB / Custom. Provisioned RDS Instance Size and EBS Volume Type & Size. Auto-scaling. Support for Read Replicas and Multi AZ. Security through IAM, Security Groups, KMS , SSL in transit. Automated Backup with Point in time restore feature (up to 35 days). Manual DB Snapshot for longer-term recovery. Managed and Scheduled maintenance (with downtime). Support for IAM Authentication, integration with Secrets Manager. RDS Custom for access to and customize the underlying instance (Oracle & SQL Server).
   - Aurora: Compatible API for PostgreSQL/MySQL,separation of storage and compute. Storage:data is stored in 6replicas,across 3AZ–highly available,self-healing,auto-scaling. Compute: Cluster of DB Instance across multiple AZ, auto-scaling of Read Replicas. Cluster: Custom endpoints for writer and reader DB instances. Same security / monitoring / maintenance features as RDS. Know the backup & restore options for Aurora. `Aurora Serverless` – for unpredictable / intermittent workloads, no capacity planning. `Aurora Multi-Master` – for continuous writes failover (high write availability). `Aurora Global`: up to 16 DB Read Instances in each region, < 1 second storage replication. `Aurora Machine Learning`: perform ML using SageMaker & Comprehend on Aurora. `Aurora Database Cloning`:new cluster from existing one, faster than restoring a snapshot. 
 - NoSQL database – no joins, no SQL : DynamoDB (~JSON), ElastiCache (key / value pairs), Neptune (graphs), DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra)
   - ElastiCache: Managed Redis / Memcached (similar offering as RDS, but for caches). In-memory data store, sub-millisecond latency. Select an ElastiCache instance type (e.g., cache.m6g.large). Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding). Security through IAM, Security Groups, KMS, Redis Auth. Backup / Snapshot / Point in time restore feature. Managed and Scheduled maintenance. Requires some application code changes to be leveraged.
   - DynamoDB: Use Case: Serverless applications development (small documents 100s KB), distributed serverless cache. Great to rapidly evolve schemas. AWS proprietary technology, managed serverless NoSQL database, millisecond latency. Capacity modes: provisioned capacity with optional auto-scaling or on-demand capacity. Can replace ElastiCache as a key/value store (storing session data for example, using TTL feature). Highly Available, Multi AZ by default, Read and Writes are decoupled, transaction capability. DAX cluster for read cache, microsecond read latency. Security, authentication and authorization is done through IAM. Event Processing: DynamoDB Streams to integrate with AWS Lambda, or Kinesis Data Streams. Global Table feature: active-active setup. Automated backups up to 35 days with PITR (restore to new table), or on-demand backups. Export to S3 without using RCU within the PITR window, import from S3 without using WCU.
   - DocumentDB: DocumentDB is the same for MongoDB (which is a NoSQL database). MongoDB is used to store, query, and index JSON data. Similar “deployment concepts” as Aurora. Fully Managed, highly available with replication across 3 AZ. DocumentDB storage automatically grows in increments of 10GB, up to 64 TB. Automatically scales to workloads with millions of requests per seconds.
   - Keyspaces (for Apache Cassandra): Apache Cassandra is an open-source NoSQL distributed database. Serverless, Scalable, highly available, fully managed by AWS. Automatically scale tables up/down based on the application’s traffic. Tables are replicated 3 times across multiple AZ. Using the Cassandra Query Language (CQL). Single-digit millisecond latency at any scale, 1000s of requests per second. Capacity: On-demand mode or provisioned mode with auto-scaling. Encryption, backup, Point-In-Time Recovery (PITR) up to 35 days. Use cases: store IoT devices info, time-series data, ...
 - Object Store: S3 (for big objects) / Glacier (for backups / archives)
   - S3: S3 is a... key / value store for objects. Great for bigger objects, not so great for many small objects. Serverless, scales infinitely, max object size is 5 TB, versioning capability. Tiers: S3 Standard, S3 Infrequent Access, S3 Intelligent, S3 Glacier + lifecycle policy. Features: Versioning, Encryption, Replication, MFA-Delete, Access Logs... Security: IAM, Bucket Policies, ACL, Access Points, Object Lambda, CORS, Object/Vault Lock. Encryption: SSE-S3, SSE-KMS, SSE-C, client-side,TLS in transit, default encryption. Batch operations on objects using S3 Batch, listing files using S3 Inventory. Performance: Multi-part upload, S3 Transfer Acceleration, S3 Select. Automation: S3 Event Notifications (SNS, SQS, Lambda, EventBridge). Use Cases: static files, key value store for big files, website hosting.
 - Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena, EMR
 - Search: OpenSearch (JSON) – free text, unstructured searches
 - Graphs: Amazon Neptune – displays relationships between data
   - Fully managed graph database. A popular graph dataset would be a social network (Users have friends, Posts have comments, Users share and like posts...). Highly available across 3 AZ, with up to 15 read replicas. Build and run applications working with highly connected datasets – optimized for these complex and hard queries. Can store up to billions of relations and query the graph with milliseconds latency. Highly available with replications across multiple AZs. Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking.
 - Ledger: Amazon Quantum Ledger Database
   - QLDB stands for ”Quantum Ledger Database”. A ledger is a book recording financial transactions. fully managed, Serverless, High available, Replication across 3 AZ. Used to review history of all the changes made to your application data over time. Immutable system: no entry can be removed or modified, cryptographically verifiable. 2-3x better performance than common ledger blockchain frameworks, manipulate data using SQL. Difference with Amazon Managed Blockchain: no decentralization component, in accordance with financial regulation rules
 - Time series:AmazonTimestream
   - Fully managed, fast, scalable, serverless time series database. Automatically scales up/down to adjust capacity. Store and analyze trillions of events per day. 1000s times faster & 1/10th the cost of relational databases. Scheduled queries, multi-measure records, SQL compatibility. Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage. Built-in time series analytics functions (helps you identify patterns in your data in near real-time). Encryption in transit and at rest. Use cases: IoT apps, operational applications, real-time analytics, ...
## Data and Analytics
 - Amazon Athena:
   - Serverless query service to analyze data stored in Amazon S3
   - Uses standard SQL language to query the files (built on Presto)
   - Supports CSV, JSON, ORC, Avro, and Parquet
   - Commonly used with Amazon Quicksight for reporting/dashboards
   - Use cases: Business intelligence / analytics / reporting, analyze & queryVPC Flow Logs,ELB Logs,CloudTrail trails,etc...
   - Performance Improvement
     - Use columnar data for cost-savings (less scan): Apache Parquet or ORC is recommended. Use Glue to convert your data to Parquet or ORC
     - Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)
     - Partition datasets in S3 for easy querying on virtual columns
     - Use larger files (> 128 MB) to minimize overhead
     - Federated Query: Allows you to run SQL queries across data stored in relational, non-relational, object, and custom data sources (AWS or on-premises). Uses Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...). Store the results back in Amazon S3.
 - Redshift Overview
   - Redshift is based on PostgreSQL, but it’s not used for OLTP
   - It’s OLAP – online analytical processing (analytics and data warehousing)
   - 10x better performance than other data warehouses, scale to PBs of data
   - Columnar storage of data (instead of row based) & parallel query engine
   - Pay as you go based on the instances provisioned
   - Has a SQL interface for performing the queries
   - BI tools such as Amazon Quicksight or Tableau integrate with it
   - vs Athena: faster queries / joins / aggregations thanks to indexes
 - Redshift Cluster
   - Leader node: for query planning, results aggregation
   - Compute node: for performing the queries, send results to leader
   - You provision the node size in advance
   - You can used Reserved Instances for cost savings
 - Redshift – Snapshots & DR
   - Redshift has “Multi-AZ” mode for some clusters
   - Snapshots are point-in-time backups of a cluster, stored internally in S3
   - Snapshots are incremental (only what has changed is saved).
   - You can restore a snapshot into a new cluster
   - Automated: every 8 hours, every 5 GB, or on a schedule. Set retention between 1 to 35 days. Manual: snapshot is retained until you delete it
   - You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region
 - Loading data into Redshift: Large inserts are MUCH better
   - Amazon Kinesis Data Firehose
   - S3 using COPY command
   - EC2 Instance JDBC driver
 - Redshift Spectrum
   - Query data that is already in S3 without loading it (Redshift cluster -- Redshift spectrum -- S3)
   - Must have a Redshift cluster available to start the query
   - The query is then submitted to thousands of Redshift Spectrum nodes
 - Amazon OpenSearch Service (Elastic Search)
   - In DynamoDB, queries only exist by primary key or indexes...
   - With OpenSearch, you can search any field, even par tially matches
   - It’s common to use OpenSearch as a complement to another database
   - Two modes: managed cluster or serverless cluster
   - Does not natively support SQL (can be enabled via a plugin)
   - Ingestion from Kinesis Data Firehose, AWS IoT, and CloudWatch Logs
   - Security through Cognito & IAM, KMS encryption,TLS
   - Comes with OpenSearch Dashboards (visualization)
 - OpenSearch patterns
   - DynamoDB (start with querying with openSearch, then use the results to retrieve items from DynamoDB. DynamoDB must enable DynamoDB stream and use a lambda function to write data into OpenSearch)
   - CloudWatch Logs (CloudWatch logs -- subscription filter -- Lambda / Kinesis Data Firehose -- OpenSearch (lambda: real-time, Kinesis: near real-time))
   - Kinesis Data Streams & Kinesis Data Firehose : can put a lambda in the middle to do data transformation
 - Amazon EMR: Elastic MapReduce
   - EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
   - The clusters can be made of hundreds of EC2 instances
   - EMR comes bundled with Apache Spark, HBase, Presto, Flink...
   - EMR takes care of all the provisioning and configuration
   - Auto-scaling and integrated with Spot instances
   - Use cases: data processing, machine learning, web indexing, big data...
 - Amazon EMR – Node types & purchasing
   - Master Node: Manage the cluster, coordinate, manage health – long running
   - Core Node: Run tasks and store data – long running
   - Task Node (optional): Just to run tasks – usually Spot
   - Purchasing options: on-demand, Reserved(min 1 yr), Spot Instances
   - Can have long-running cluster, or transient (temporary) cluster
 - Amazon QuickSight: Serverless machine learning-powered business intelligence service to create interactive dashboards
   - Fast, automatically scalable, embeddable, with per-session pricing
   - Use cases: Business analytics, Building visualizations, Perform ad-hoc analysis, Get business insights using data
   - Integrated with RDS, Aurora, Athena, Redshift, S3...
   - In-memory computation using SPICE engine if data is imported into QuickSight
   - Enterprise edition: Possibility to setup Column-Level security (CLS)
   - QuickSight Integrations: Aws services, SaaS, import files, on-prem
   - Dashboard & Analysis:
     - Define Users (standard versions) and Groups (enterprise version), These users & groups only exist within QuickSight, not IAM !!
     - A dashboard is a read-only snapshot of an analysis that you can share, preserves the configuration of the analysis (filtering, parameters, controls, sort).
     - You can share the analysis or the dashboard with Users or Groups
     - To share a dashboard, you must first publish it
     - Users who see the dashboard can also see the underlying data
 - AWS Glue
   - Managed extract, transform, and load (ETL) service
   - Useful to prepare and transform data for analytics
   - Fully serverless service
   - Convert data into Parquet format(better format for Athena to query data): using Lambda or eventbridge to trigger aws glue ETL job
   - Glue Data Catalog: catalog of datasets
     - AWS Glue Data Crawler: write metadata of your data sources into AWS Glue Data Catalog tables(different schemas), then services like Athena, redshift spectrum, or EMR will leverage those tables
   - Glue Job Bookmarks: prevent re-processing old data
   - Glue Elastic Views: Combine and replicate data across multiple data stores using SQL. No custom code, Glue monitors for changes in the source data, serverless. Leverages a “virtual table” (materialized view).
   - Glue DataBrew: clean and normalize data using pre-built transformation
   - Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
   - Glue Streaming ETL (built on Apache Spark Structured Streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)
 - AWS Lake Formation
   - Data lake = central place to have all your data for analytics purposes
   - Fully managed service that makes it easy to setup a data lake in days
   - Discover, cleanse, transform, and ingest data into your Data Lake
   - It automates many complex manual steps (collecting, cleansing, moving, cataloging data, ...) and de-duplicate (using ML Transforms)
   - Combine structured and unstructured data in the data lake
   - Out-of-the-box source blueprints: S3, RDS, Relational & NoSQL DB...
   - Fine-grained Access Control for your applications (row and column-level)
   - Built on top of AWS Glue
 - Kinesis Data Analytics (SQL application)
   - Real-time analytics on Kinesis Data Streams & Firehose using SQL
   - Add reference data from Amazon S3 to enrich streaming data
   - Fully managed, no servers to provision, Automatic scaling
   - Pay for actual consumption rate
   - Output:
     - Kinesis Data Streams: create streams out of the real-time analytics queries
     - Kinesis Data Firehose: send analytics query results to destinations
   - Use cases: Time-series analytics, Real-time dashboards, Real-time metrics
 - Kinesis Data Analytics for Apache Flink (sources: Kinesis Data Streams & Amazon MSK)
   - Use Flink (Java, Scala or SQL) to process and analyze streaming data
   - Run any Apache Flink application on a managed cluster on AWS
     - provisioning compute resources, parallel computation, automatic scaling
     - application backups (implemented as checkpoints and snapshots)
     - Use any Apache Flink programming features
     - Flink does not read from Firehose (use Kinesis Analytics for SQL instead)
 - Amazon Managed Streaming for Apache Kafka (Amazon MSK)
   - Alternative to Amazon Kinesis
   - Fully managed Apache Kafka on AWS
     - Allow you to create, update, delete clusters
     - MSK creates & manages Kafka brokers nodes & Zookeeper nodes for you
     - Deploy the MSK cluster in your VPC, multi-AZ (up to 3 for HA)
     - Automatic recovery from common Apache Kafka failures
     - Data is stored on EBS volumes for **as long as you want**
   - MSK Serverless
     - Run Apache Kafka on MSK without managing the capacity
     - MSK automatically provisions resources and scales compute & storage
 - Kinesis Data Streams vs. Amazon MSK
   - Kinesis Data Streams:
     - 1 MB message size limit
     - Data Streams with Shards
     - Shard Splitting & Merging
     - TLS In-flight encryption
     - KMS at-rest encryption
   - Amazon MSK
     - 1MB default, configure for higher (ex: 10MB)
     - KafkaTopics with Partitions
     - Can only add partitions to a topic
     - PLAINTEXT orTLS In-flight Encryption
     - KMS at-rest encryption
 - Amazon MSK Consumers: Kinesis Data Analytics for Apache Flink, AWS Glue Streaming ETL Jobs Powered by Apache Spark Streaming, Lambda, or apps on EC2 instances, ECS, EKS
 - Big Data Ingestion Pipeline: Kinesis Data Streams --> Kinesis Data Firehose(lambda for transformation)-->S3-->SQS(optional)-->lambda-->Athena-->pull from S3-->put into S3(target)-->Redshift(not serverless)/Aws quicksight
## Machine Learning
 - Rekognition: face detection, content moderation(Set a **Minimum Confidence Threshold** for items that will be flagged. Flag sensitive content for manual review in
Amazon Augmented AI (A2I)), labeling, celebrity recognition -- detect objects, people, text, and scenes in images or videos
 - Transcribe:  convert speech to text (ex: subtitles). 1)Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately. 2)Automatically remove Personally Identifiable Information (PII) using Redaction. 3)Supports Automatic Language Identification for multi-lingual audio.
 - Polly: text to lifelike speech using deep learning. **Lexicon(Customize the pronunciation of words with Pronunciation lexicons) &  Speech Synthesis Markup Language (SSML, more customization)**
 - Translate: translations,  localize content
 - Lex: build conversational bots – chatbots
 - Connect:  cloud-based virtual contact center
 - Comprehend: natural language processing(NLP). Analyzing text to find insights and relationships. **Amazon Comprehend Medical**: detects and returns useful information in
unstructured clinical text. Uses NLP to detect **Protected Health Information (PHI)** – DetectPHI API.
 - SageMaker: machine learning for every developer and data scientist, easy-to-train models.
 - Forecast: build highly accurate forecasts -- feeding data and then training the model, last using the model to predict
 - Kendra: ML-powered search engine -- extract and index contents from documents of different formats, and then make the results indexable.
 - Personalize: real-time personalized recommendations, integrates into existing websites, apps, SMS, email marketing system
 - Textract: detect text and data in any scanned documents of any type
## AWS Monitoring Audit and Performance
 - Amazon CloudWatch Metrics: metrics for every service in AWS. Metrics belong to namespaces. Dimension is an attribute of a metric(up to 30). Metrics have timestamps. Custom metrics can be created.
 - CloudWatch Metric Streams: Continually stream CloudWatch metrics to a destination of your choice, with **near-real-time** delivery and low latency (Kinesis Data Firehose, or 3rd party services, such as datadog). Optionally, **filter metrics** to only stream a subset of them.
 - CloudWatch Logs: Log groups representing an application. log stream(instance in apps or containers). destinations: s3, opensearch, lambda, kinesis data streams, kinesis data firehose. Encrypted by default, can be encrypted using custom key.
 - CloudWatch logs sources: SDK, CloudWatch Logs Agent, CloudWatch Unified Agent, Elastic Beanstalk, ECS, AWS Lambda, VPC Flow Logs, API Gateway, CloudTrail, Route53.
 - CloudWatch Logs Insights: Search and analyze log data stored in CloudWatch Logs. Provides a purpose-built query language. Can query multiple Log Groups in different AWS accounts. It’s a query engine, not a real-time engine.
 - CloudWatch Logs – S3 Export: take up to 12 hrs. API call(CreateExportTask). Not near-real-time or real-time… use Logs Subscriptions instead.
 - CloudWatch Logs Subscriptions:  get real-time logs from cloudwatch, and send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda. **Subscription Filter(filter logs)**.
 - CloudWatch Logs AggregationMulti-Account & Multi-Region: [account1/region1...] -> [Subscription Filter...] -> Kinesis Data Streams -> Kinesis Data Firehose -> S3.
 - CloudWatch Logs Subscriptions: Cross-Account Subscription – send log events to resources in a different AWS account (KDS, KDF) (**in this case, we need modify the destination cloudwatch service's access policy to accept the logs from the original cloudwatch service. meanwhile, we need a IAM role to allow kinesis data streams to collect logs**)
 - CloudWatch Logs for EC2: we need **cloudwatch logs agent** running on the EC2 instances with proper IAM permissions.
 - CloudWatch Logs Agent & Unified Agent: logs agent -- old version, can only send logs to cloudwatch. unified agent -- collect and send logs as well as additional system-level metrics such as RAM, CPU. Centralized configuration using SSM Parameter Store.
 - CloudWatch Unified Agent – Metrics : CPU, RAM, DISK metrics, Netstat, processes, swap space(free, used)
 - CloudWatch Alarms: used to trigger notifications for any metrics
   - Various options (sampling, %, max, min, etc…)
   - Alarm States: OK, INSUFFICIENT_DATA, ALARM
   - Period: Length of time in seconds to evaluate the metric (High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec)
   - target: EC2, ASG, SNS
 - CloudWatch Alarms – Composite Alarms: CloudWatch Alarms are on a single metric. Composite Alarms are monitoring the states of multiple other alarms. AND and OR conditions. Helpful to reduce “alarm noise” by creating complex composite alarms.
 - EC2 Instance Recovery:
   - status checks: instance status and system status
 - Good to know: 1)Alarms can be created based on CloudWatch Logs Metrics Filters. 2)to test alarms and notifications, we can use CLI to manually change alarm state.
 - Amazon EventBridge:
   - schedule: cron jobs
   - event pattern: Event rules to react to a service doing something
   - trigger lambda, SNS/SQS, ...
   - can also use event filter
   - Event buses can be accessed by other AWS accounts using Resource-based Policies
   - You can archive events (all/filter) sent to an event bus (indefinitely or set period)
   - Ability to replay archived events
 - Amazon EventBridge – Schema Registry
   - EventBridge can analyze the events in your bus and infer the schema
   - The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus
   - Schema can be versioned
 - Amazon EventBridge – Resource-based Policy
   - Manage permissions for a specific Event Bus
   - Example: allow/deny events from another AWS account or AWS region
 - CloudWatch Container Insights
   - Collect, aggregate, summarize metrics and logs from containers
   - containers in ECS, EKS, K8S on EC2, Fargate
   - In Amazon EKS and Kubernetes, CloudWatch Insights is using a containerized version of the CloudWatch Agent to discover containers
 - CloudWatch Lambda Insights
   - Monitoring and troubleshooting solution for serverless applications running on AWS Lambda
   - collect system-level metrics, such as CPU, diagnostic information, such as cold start
   - Lambda Insights is provided as a Lambda Layer
 - CloudWatch Contributor Insights
   - Analyze log data and create time series that display contributor data. (See metrics about the top-N contributors)
   - For example, you can find bad hosts, identify the heaviest network users, or find the URLs that generate the most errors.
   - You can build your rules from scratch, or you can also use sample rules that AWS has created – leverages your CloudWatch Logs
 - CloudWatch Application Insights (powered by sageMaker)
   - Provides automated dashboards that show potential problems with monitored applications, to help isolate ongoing issues
   - Findings and alerts are sent to Amazon EventBridge and SSM OpsCenter
 - AWS CloudTrail
   - Provides governance, compliance and audit for your AWS Account
   - CloudTrail is enabled by default!
   - Get an history of events / API calls made within your AWS Account. Can put logs from CloudTrail into CloudWatch Logs or S3
   - A trail can be applied to All Regions (default) or a single Region.
   - If a resource is deleted in AWS, investigate CloudTrail first!
 - CloudTrail Events
   - Management Events: By default, trails are configured to log management events.
     - Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)
     - Operations that are performed on resources in your AWS account
   - Data Events: By default, data events are not logged (because high volume operations)
   - CloudTrail Insights Events: Enable CloudTrail Insights to detect unusual activity in your account
     - CloudTrail Insights analyzes normal management events to create a baseline
     - And then continuously analyzes write events to detect unusual patterns, send insight events to S3, eventBridge, or cloudtrail console
 - CloudTrail Events Retention: 90 days in cloudtrail, or send to S3 and use Athena for long-term storage
 - CloudTrail + EventBridge
 - AWS Config:  a per-region service
   - Can be aggregated across regions and accounts
   - Helps with auditing and recording compliance of your AWS resources. Helps record configurations and changes over time
   - You can receive alerts (SNS notifications) for any changes
   - Possibility of storing the configuration data into S3 (analyzed by Athena)
 - Config Rules
   - Can use AWS managed config rules (over 75)
   - Can make custom config rules (must be defined in AWS Lambda)
   - Rules can be evaluated / triggered: for each configuration change or at regular intervals
   - AWS Config Rules does not prevent actions from happening (no deny)
   - pricing: no free tier
 - Config Rules – Remediations
   - Automate remediation of non-compliant resources using SSM Automation Documents
   - Use AWS-Managed Automation Documents or create custom Automation Documents
   - You can set Remediation Retries if the resource is still non-compliant after auto-remediation
 - Config Rules – Notifications
   - Use EventBridge to trigger notifications when AWS resources are non-compliant
   - Ability to send configuration changes and compliance state notifications to SNS (all events – use SNS Filtering or filter at client-side)
 - CloudWatch vs CloudTrail vs Config
   - CloudWatch:
     - Performance monitoring (metrics, CPU, network, etc…) & dashboards
     - Events & Alerting
     - Log Aggregation & Analysis
   - CloudTrail:
     - Record API calls made within your Account by everyone
     - Can define trails for specific resources
     - Global Service
   - Config
     - Record configuration changes
     - Evaluate resources against compliance rules
     - Get timeline of changes and compliance
 - For an Elastic Load Balancer (Example)
   - CloudWatch:
     - Monitoring Incoming connections metric
     - Visualize error codes as % over time
     - Make a dashboard to get an idea of your load balancer performance
   - CloudTrail:
     - Track who made any changes to the Load Balancer with API calls
   - Config:
     - Track security group rules for the Load Balancer
     - Track configuration changes for the Load Balancer
     - Ensure an SSL certificate is always assigned to the Load Balancer (compliance)
## AWS IAM Advanced
 - AWS Organizations
   - Global service
   - Allows to manage multiple AWS accounts
   - The main account is the management account
   - Other accounts are member accounts
   - Member accounts can only be part of one organization
   - Consolidated Billing across all accounts - single payment method
   - Pricing benefits from aggregated usage (volume discount for EC2, S3...)
   - Shared reserved instances and Savings Plans discounts across accounts
   - API is available to automate AWS account creation
   - organizations can be divided by business unit, Environmental Lifecycle, Project-Based
   - Advantages:
     - Multi Account vs One Account Multi VPC
     - Use tagging standards for billing purposes
     - Enable CloudTrail on all accounts, send logs to central S3 account
     - Send CloudWatch Logs to central logging account
     - Establish Cross Account Roles for Admin purposes
   - Security: Service Control Policies (SCP)
     - IAM policies applied to OU or Accounts to restrict Users and Roles
     - **They do not apply to the management account (full admin power)**
     - **Must have an explicit allow (does not allow anything by default – like IAM)**
 - IAM conditions
   - aws:SourceIp --> restrict the client IP from which the API calls are being made
   - aws:RequestedRegion --> restrict the region the API calls are made to
   - ec2:ResourceTag --> restrict based on tags
   - aws:MultiFactorAuthPresent --> to force MFA
 - IAM for S3
   - bucket level permission
   - object level permission
 - Resource Policies & aws:PrincipalOrgID
   - aws:PrincipalOrgID can be used in any resource policies to restrict access to accounts that are member of an AWS Organization
 - IAM Roles vs Resource Based Policies
   - Cross account:
     - attaching a resource-based policy to a resource (example: S3 bucket policy)
     - OR using a role as a proxy
   - When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role
   - When using a resource-based policy, the principal doesn’t have to give up his permissions
   - Example: User in account A needs to scan a DynamoDB table in Account A and dump it in an S3 bucket in Account B.
   - Suppor ted by: Amazon S3 buckets, SNS topics, SQS queues, etc...
 - Amazon EventBridge – Security
   - When a rule runs, it needs permissions on the target
   - Resource-based policy: Lambda, SNS, SQS, CloudWatch Logs, API Rule Gateway...
   - IAM role: Kinesis stream, Systems Manager Run Command, ECS task...
 - IAM Permission Boundaries
   - **IAM Permission Boundaries are supported for users and roles (not groups)**
   - Advanced feature to use a managed policy to set the maximum permissions an IAM entity can get.
   - Can be used in combinations of AWS Organizations SCP
   - use cases:
     - Delegate responsibilities to non administrators within their permission boundaries, for example create new IAM users
     - Allow developers to self-assign policies and manage their own permissions, while making sure they can’t “escalate” their privileges (= make themselves admin)
     - Useful to restrict one specific user (instead of a whole account using Organizations & SCP)
 - IAM Policy Evaluation Logic: once there's a **deny**, then it's **deny** no matter what. Otherwise, there has to be explicit **allow**
 - AWS IAM Identity Center (successor to AWS Single Sign-On)
   - **note:** the permission set will be combined with the permissions inherited from the AWS account as a supplement not a replacement, providing more granular access control.
   - One login (single sign-on) for all your
     - **AWS accounts in AWS Organizations**
     - Business cloud applications (e.g., Salesforce, Box, Microsoft 365, ...)
     - SAML2.0-enabled applications
     - EC2 Windows Instances
   - Identity providers
     - Built-in identity store in IAM Identity Center
     - 3rd party: Active Directory (AD), OneLogin, Okta...
 - AWS IAM Identity Center Fine-grained Permissions and Assignments
   - Multi-Account Permissions: Manage access across AWS accounts in your AWS Organization. Permission Sets – a collection of one or more IAM Policies assigned to users and groups to define AWS access.
   - Application Assignments: SSO access to many SAML 2.0 business applications (Salesforce, Box, Microsoft 365, ...). Provide required URLs, certificates, and metadata
   - Attribute-Based Access Control (ABAC)
 - What is Microsoft Active Directory (AD)?
   - Found on any Windows Server with AD Domain Services
   - Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups
   - Centralized security management, create account, assign permissions
   - Objects are organized in trees
   - A group of trees is a forest
 - AWS Directory Services
   - AWS Managed Microsoft AD: create your own AD in AWS, manage users locally, support MFA. Establish “trust” connections with your on- premises AD
   - AD Connector: Directory Gateway (proxy) to redirect to on-premises AD, supports MFA. Users are managed on the on-premises AD
   - Simple AD: AD-compatible managed directory on AWS. Cannot be joined with on-premises AD
 - IAM Identity Center – Active Directory Setup
   - Connect to an AWS Managed Microsoft AD (Directory Service): Integration is out of the box
   - Connect to a Self-Managed Directory: Create Two-way Trust Relationship using AWS Managed Microsoft AD. or Create an AD Connector
 - AWS Control Tower
   - Easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices
   - AWS Control Tower uses AWS Organizations to create accounts
   - Benefits:
     - Automate the set up of your environment in a few clicks
     - Automate ongoing policy management using guardrails
     - Detect policy violations and remediate them
     - Monitor compliance through an interactive dashboard
 - AWS Control Tower – Guardrails
   - `Provides ongoing governance for your Control Tower environment (AWS Accounts)`
   - **Preventive Guardrail – using SCPs (e.g., Restrict Regions across all your accounts)**
   - **Detective Guardrail – using AWS Config (e.g., identify untagged resources)**
## AWS Security and Encryption
 - Encryption in flight (TLS / SSL): Data is encrypted before sending and decrypted after receiving
 - Server-side encryption at rest: Data is encrypted after being received by the server and decrypted before being sent. The encryption/decryption keys must be managed somewhere, and the server must have access to it
 - Client-side encryption: Data is encrypted by the client and never decrypted by the server, Data will be decrypted by a receiving client
 - AWS KMS (Key Management Service): manages encryption keys for us, fully integrated with IAM for authorization, easy way to control access to your data
   - Able to audit KMS Key usage using CloudTrail
   - Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM…)
   - Never ever store your secrets in plaintext, especially in your code! Use the KMS key to encrypt your secrets so that they can be stored in the code or config file.
 - KMS Keys Types
   - Symmetric (AES-256 keys): must call KMS API to use
   - Asymmetric (RSA & ECC key pairs): Public (Encrypt) and Private Key (Decrypt) pair
   - AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)
   - AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs)
   - Customer managed keys
   - Automatic Key rotation:
     - AWS-managed KMS Key: automatic every 1 year
     - Customer-managed KMS Key: (must be enabled) automatic every 1 year
     - Imported KMS Key: only manual rotation possible using alias
 - Copying Snapshots across regions: have a snapshot encrypted with key1, then copy it and encrypt it with key2 in another region.
 - KMS Key Policies (similar to S3 bucket policy, which is a resource access policy), the difference is without this policy no one can access it.
   - Default KMS Key Policy: created if customers don't provide one, and allows anyone to access it.
   - Custom KMS Key Policy: define users, roles who can access it, and who can administer it. useful for cross-account access of your key.
   - Copying Snapshots across accounts:
     - Create a Snapshot, encrypted with your own KMS Key (Customer Managed Key)
     - Attach a KMS Key Policy to authorize cross-account access
     - Share the encrypted snapshot
     - (in target) Create a copy of the Snapshot, encrypt it with a CMK in your account
     - Create a volume from the snapshot
 - KMS Multi-Region Keys
   - Identical KMS keys in different AWS Regions that can be used interchangeably
   - No need to re-encrypt or making cross-Region API calls
   - KMS Multi-Region are NOT global (Primary + Replicas)
   - Each Multi-Region key is managed independently, each has their own key policies
   - Use cases: global client-side encryption, encryption on Global DynamoDB ( encrypt specific attributes client-side using Amazon DynamoDB Encryption Client), Global Aurora(encrypt specific attributes client-side using AWS Encryption SDK, we can protect specific fields even from database admins)
 - S3 Replication Encryption Considerations
   - Unencrypted objects and objects encrypted with SSE-S3 are replicated by default
   - Objects encrypted with SSE-C (customer provided key) can be replicated
   - For objects encrypted with SSE-KMS, you need to enable the option
     - Specify which KMS Key to encrypt the objects within the target bucket
     - Adapt the KMS Key Policy for the target key
     - An IAM Role with kms:Decrypt for the source KMS Key and kms:Encrypt for the target KMS Key for S3 replication service
     - You might get KMS throttling errors, in which case you can ask for a Service Quotas increase
   - You can use multi-region AWS KMS Keys, but they are currently treated as independent keys by Amazon S3 (the object will still be decrypted and then encrypted)
 - AMI Sharing Process Encrypted via KMS (cross-account)
   - AMI in Source Account is encrypted with KMS Key from Source Account
   - Must modify the image attribute to add a Launch Permission which corresponds to the specified target AWS account
   - Must share the KMS Keys used to encrypted the snapshot the AMI references with the target account / IAM Role
   - The IAM Role/User in the target account must have the permissions to DescribeKey, ReEncrypted, CreateGrant, Decrypt
   - When launching an EC2 instance from the AMI, optionally the target account can specify a new KMS key in its own account to re-encrypt the volumes
 - SSM Parameter Store: Secure storage for configuration and secrets, optionally can use KMS for encryption, version tracking, security through IAM, notification from EventBridge, integration with CloudFormation
 - SSM Parameter Store Hierarchy: (eg. /awef/awfea/wafw... )
 - SSM Parameter tier (standard & advanced)
   - Parameters Policies (for advanced parameters)
     - Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords
     - Can assign multiple policies at a time
 - AWS Secrets Manager: meant for storing secrets
   - Capability to force rotation of secrets every X days
   - Automate generation of secrets on rotation (uses Lambda)
   - Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)
   - Secrets are encrypted using KMS
   - Mostly meant for RDS integration
 - AWS Secrets Manager – Multi-Region Secrets
   - Replicate Secrets across multiple AWS Regions
   - Secrets Manager keeps read replicas in sync with the primary Secret
   - Ability to promote a read replica Secret to a standalone Secret
   - Use cases: multi-region apps, disaster recovery strategies, multi-region DB…
 - AWS Certificate Manager (ACM)
   - Easily provision, manage, and deploy TLS Certificates
   - Supports both public( free) and private TLS certificates
   - Automatic TLS certificate renewal
   - Cannot use ACM with EC2 (can’t be extracted)
   - Integrations with ELB, CloudFront, API Gateway
 - ACM – Requesting Public Certificates
   - List domain names to be included in the certificate
   - Select Validation Method: DNS Validation or Email validation
   - It will take a few hours to get verified
   - The Public Certificate will be enrolled for automatic renewal (60 days before expiry)
 - ACM – Importing Public Certificates
   - Option to generate the certificate outside of ACM and then import it
   - No automatic renewal
   - ACM sends daily expiration events
   - AWS Config has a managed rule named acm-certificate-expiration-check to check for expiring certificates (configurable number of days)
 - ACM – Integration with ALB
 - API Gateway - Endpoint Types
   - Edge-Optimized (default)
   - Regional
   - Private
 - ACM – Integration with API Gateway
   - Create a Custom Domain Name in API Gateway
   - Edge-Optimized (default): The TLS Certificate must be in the same region as CloudFront, in us-east-1. The API Gateway still lives in only one region. Then setup CNAME or (better) A-Alias record in Route 53
   - Regional: The TLS Certificate must be imported on API Gateway, in the same region as the API Stage. Then setup CNAME or (better) A-Alias record in Route 53.
 - AWS WAF – Web Application Firewall: Protects your web applications from common web exploits (Layer 7 HTTP)
   - deploy on: ALB, API Gateway, CloudFront, AppSync GraphQL API, Cognito User Pool
   - Define Web ACL (Web Access Control List) Rules: IP set, SQL injection and XSS, Size constraints, geo-match (block countries), Rate-based rules (to count occurrences of events) – for DDoS protection
   - Web ACL are Regional except for CloudFront
   - A rule group is a reusable set of rules that you can add to a web ACL
 - WAF – Fixed IP while using WAF with a Load Balancer
   - ALB doesn't have a fixed IP, but we can use Global Accelerator with fixed IPs, then apply WAF onto ALB
 - AWS Shield: protect from DDoS attack:
   - DDoS: Distributed Denial of Service – many requests at the same time
   - AWS Shield Standard: free and activated for every AWS customer
   - AWS Shield Advanced:  automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks. Optional DDoS mitigation service. Protect against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53.
 - AWS Firewall Manager: Manage rules in all accounts of an AWS Organization
   - Security policy: common set of security rules
   - Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization
 - WAF vs. Firewall Manager vs. Shield
   - WAF, Shield and Firewall Manager are used together for comprehensive protection
   - Define your Web ACL rules in WAF, For granular protection of your resources, WAF alone is the correct choice
   - use Firewall Manager with AWS WAF to apply protection across accounts
   - Shield Advanced adds additional features on top of AWS WAF
   - If you’re prone to frequent DDoS attacks, consider purchasing Shield Advanced
 - AWS Best Practices for DDoS Resiliency
   - Edge Location Mitigation:
     - CloudFront: natural defense to DDoS
     - Global Accelerator: integrate with AWS Shield to defend DDoS
     - Route 53: Domain Name Resolution to defend DDoS
   - Best pratices for DDoS mitigation
     - Infrastructure layer defense: using Global Accelerator, Route 53, CloudFront, Elastic Load Balancing
     - Amazon EC2 with Auto Scaling: in case traffic reach EC2 instances
     - Elastic Load Balancing: scales along with incoming traffic and distribute the traffic
   - Application Layer Defense
     - Detect and filter malicious web requests
       - CloudFront cache and block specific geographies
       - WAF on CloudFront and ELB to filter and block certain requests, also the rate-based rules and managed rules can be used to block certain IPs
     - Shield Advanced:  automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks
   - Attack surface reduction
     - Obfuscating AWS resources: use CloudFront, ELB, and API Gateway to hide backend
     - Security groups and Network ACLs: Use security groups and NACLs to filter traffic based on specific IP at the subnet or ENI-level. Elastic IP are protected by AWS Shield Advanced
     - Protecting API endpoints: hide backend, Edge-optimized mode, or CloudFront + regional mode (more control for DDoS), WAF + API Gateway: burst limits, headers filtering, use API keys
 - Amazon GuardDuty: Intelligent Threat discovery to protect your AWS Account
   - input data: CloudTrail Events Logs, VPC Flow Logs, DNS Logs, Optional Features(other services logs or events)
   - can setup EventBridge rules
   - Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)
 - Amazon Inspector: Automated Security Assessments on
   - EC2 instances
   - Container Images push to Amazon ECR
   - Lambda Functions
   - Reporting & integration with AWS Security Hub, Send findings to Amazon Event Bridge
   - What does Amazon Inspector evaluate? Remember: only for EC2 instances, Container Images & Lambda functions
     - Continuous scanning of the infrastructure, only when needed
     - Package vulnerabilities (EC2, ECR & Lambda) – database of CVE
     - Network reachability (EC2)
     - A risk score is associated with all vulnerabilities for prioritization
 - AWS Macie
   - a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.
   - Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)

## AWS VPC
 - Understanding CIDR (Classless Inter-Domain Routing) – IPv4: Base IP and Subnet Mask
 - private vs public IP (IPV4)
   - private IP: 10.0.0.0/8 (big networks), 172.16.0.0/12 (AWS default VPC), 192.168.0.0/16 (home networks)'
   - All the rest of the IP addresses on the Internet are public
 - Default VPC: new EC2 instances launched in default VPC if no subnet specified, has internet connectivity and ec2 instances have public ips, private or public DNS names.
 - VPC in AWS – IPv4
   - multiple VPCs in an AWS region (max. 5 per region – soft limit)
   - Max. CIDR per VPC is 5, for each CIDR:
     - Min. size is /28 (16 IP addresses)
     - Max. size is /16 (65536 IP addresses)
   - Because VPC is private, only the Private IPv4 ranges are allowed
   - Your VPC CIDR should **NOT overlap** with your other networks (e.g.,corporate)
 - VPC – Subnet (IPv4)
   - AWS reserves 5 IP addresses (first 4 & last 1) in each subnet
 - Internet Gateway (IGW): at the VPC level, provide IPv4 & IPv6 Internet Access
 - Route Tables: must be edited to add routes from subnets to the IGW, VPC Peering Connections, VPC Endpoints, …
 - Bastion Host:  public EC2 instance to SSH into, that has SSH connectivity to EC2 instances in private subnets
 - NAT Instances: gives Internet access to EC2 instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag
 - NAT Gateway – managed by AWS, provides scalable Internet access to private EC2 instances, IPv4 only
 - NACL – stateless, subnet rules for inbound and outbound, don’t forget Ephemeral Ports
 - Security Groups – stateful, operate at the EC2 instance level
 - VPC Peering – connect two VPCs with non overlapping CIDR, non-transitive
 - VPC Endpoints: Interface endpoint( Provisions an ENI (private IP address) as an entrypoint (must attach a Security Group), S2S VPN, Direct Connect, different VPC) , Gateway endpoint( free, S3 and DynamoDB)
 - VPC Flow Logs: can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Logs Insights
 - Site-to-Site VPN – setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet
 - AWS VPN CloudHub – hub-and-spoke VPN model to connect your sites
 - Direct Connect – setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location
 - Direct Connect Gateway – setup a Direct Connect to many VPCs in different AWS regions
 - AWS PrivateLink / VPC Endpoint Services:
   - Connect services privately from your service VPC to customers VPC
   - Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables
   - Must be used with Network Load Balancer & ENI
 - ClassicLink – connect EC2-Classic EC2 instances privately to your VPC
 - Transit Gateway – transitive peering connections for VPC, VPN & DX
 - Traffic Mirroring – copy network traffic from ENIs for further analysis
 - Egress-only Internet Gateway – like a NAT Gateway, but for IPv6 (doesn't allow the outside network to initialize the connection first)
 - Networking Costs in AWS per GB - Simplified
   - Use Private IP instead of Public IP for good savings and better network performance
   - Use same AZ for maximum savings (at the cost of high availability)
   - free for traffic-in and server-private-server(or EC2) in the same AZ, cost a little for AZ-1 -private- AZ-2, cost a bit more for AZ-1 -public- AZ-2, cost more for Region-1 -public- REgion-2
 - Minimizing egress traffic network cost
   - Try to keep as much internet traffic within AWS to minimize costs
   - inbound traffic free
   - Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network
 - S3 Data Transfer Pricing – Analysis for USA
   - ingress free
   - to internet : 0.09$/GB
   - transfer acceleration with edge locations: +0.04$/GB - 0.08$/GB
   - cloudFront: 0.085$/GB (slightly cheaper): caching, reduce cost associated with S3 requests
   - cross-region: 0.02$/GB
 - Pricing: NAT Gateway vs Gateway VPC Endpoint
   - for private subnet EC2 instances, VPC endpoint can save a lot (for S3, using gateway endpoint:free, only charge for data transfer)
 - Network Protection on AWS
   - Network Access Control Lists (NACLs)
   - Amazon VPC security groups
   - AWS WAF (protect against malicious requests)
   - AWS Shield & AWS Shield Advanced
   - AWS Firewall Manager (to manage them across accounts)
   - AWS Network Firewall (VPC)
     - From Layer 3 to Layer 7 protection: VPC-VPC, inbound/outbound, VPC-S2S VPN| Direct Connect
     - can be centrally managed cross-account by AWS Firewall Manager to apply to many VPCs
     - supports 1000 rules: IP &port, Protocol, stateful domain list, general pattern using regex
     - Traffic filtering: Allow, drop, or alert for the traffic that matches the rules
     - Active flow inspection to protect against network threats with intrusion-prevention capabilities (like Gateway Load Balancer, but all managed by AWS)
     - Send logs of rule matches to Amazon S3, CloudWatch Logs, Kinesis Data Firehose
## Disaster Recovery and Migration
 - Overview:
   - Disaster recovery (DR) is about preparing for and recovering from a disaster
   - types of DR:
     - On-premise => On-premise: traditional DR, and very expensive
     - On-premise => AWS Cloud: hybrid recovery
     - AWS Cloud Region A => AWS Cloud Region B
   - RPO(Recovery Point Objective -- data lose) and RTO (Recovery Time Objective -- down time)
   - Disaster Recovery Strategies
     - Backup and Restore (High RPO, either AWS storage gateway and snow family or schedule regular snapshots)
     - Pilot Light (core functionality is always running, such as database) -- data replication
     - Warm Standby (minimal version of full system is running, quickly to scale up ) -- data replication
     - Hot Site / Multi Site Approach (very low RTO, pricy, full production scale is always running on-prem and AWS)
   - DR Tips:
     - Backup
       - EBS or RDS snapshots/backups, etc
       - regular push to S3/ S3 IA/ Glacier, lifecycle policy, cross-region replication
       - from on-premise: snowball or storage gateway
     - High Availability
       - use Route53 to migrate DNS from region to region
       - RDS multi-AZ, elastiCache multi-AZ, EFS, S3
       - S2S VPN as failover for Direct Connect
     - Replication
       - RDS cross-region replica, Aurora + global database
       - storage gateway
       - on-prem DB to RDS
     - Automation
       - CloudFormation/ Elastic Beanstalk to re-create a whole new env
       - recover EC2 instances when Cloudwatch alarms
       - Lambda functions for customized automation
     - chaos
       - Netflix has a “simian-army” randomly terminating EC2
 - DMS – Database Migration Service: Quickly and securely migrate databases to AWS, resilient, self-healing
   - The source database remains available during the migration
   - an EC2 instance is a must for running DMS
   - Continuous Data Replication using CDC
   - supports: Homogeneous migrations / Heterogeneous migrations
   - Source and Target: take a DB and export or import it onto any DBs that AWS supports
   - AWS Schema Conversion Tool (SCT)
     - Convert your Database’s Schema from one engine to another
     - You do not need to use SCT if you are migrating the same DB engine
   - DMS - Continuous Replication: install and setup SCT(schema convertion tool) on-prem to convert the schema, and have an EC2 instance with DMS and CDC(change data capture) installed to do continuous replication
   - AWS DMS – Multi-AZ Deployment
     - When Multi-AZ Enabled, DMS provisions and maintains a synchronous stand replica in a different AZ
     - Advantages:
       - provider data redundancy
       - eliminate I/O freeze
       - minimize latency spike
 - RDS & Aurora MySQL Migrations
   - RDS MySQL to Aurora MySQL
     - Option 1: DB Snapshots from RDS MySQL restored as MySQL Aurora DB
     - Option 2: Create an Aurora Read Replica from your RDS MySQL, and when the replication lag is 0, promote it as its own DB cluster (can take time and cost $)
   - External MySQL to Aurora MySQL
     - Option 1: use Percona Xtrabackup to create a backup file in S3, then import the file into Aurora
     - Option 2: create Aurora mysql DB, then use the **mysqldump** utility to migrate Mysql to Aurora (slower than S3)
   - Use DMS if both databases are up and running
 - RDS & Aurora PostgreSQL Migrations
   - RDS PostgreSQL to Aurora PostgreSQL : similar to how migrate RDS mysql to Aurora myssql
   - External PostgreSQL to Aurora PostgreSQL: create a backup file in S3, then import it into Aurora postgres using **aws_s3 Aurora extension**
   - Use DMS if both databases are up and running
 - On-Premise strategy with AWS
   - Ability to download Amazon Linux 2 AMI as a VM (.iso format): VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V.
   - VM Import / Export: Migrate existing applications into EC2. Create a DR repository strategy for your on-premises VMs. Can export back the VMs from EC2 to on-premises.
   - AWS Application Discovery Service: gather info on on-prem servers to plan a migration. Server utilization and dependency mappings. Track with AWS Migration Hub
   - AWS Database Migration Service (DMS): works with various DBs, can do on-prem <--> AWS, AWS <--> AWS
   - AWS Server Migration Service (SMS): Incremental replication of on-premises live servers to AWS
 - AWS Backup (fully managed, no need for custom script or manual process)
   - Centrally manage and automate backups across AWS services
   - support cross-account / cross-region backups
   - support many services
   - Supports PITR for supported services
   - On-Demand and Scheduled backups
   - Tag-based backup policies
   - You create backup policies known as Backup Plans: backup frequency, backup window, transition to cold storage, retention period
 - AWS Backup Vault Lock (Backup Vault Lock Policy)
   - Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault
   - Additional layer of defense to protect your backups against
   - Even the root user cannot delete backups when enabled
 - AWS Application Discovery Service + AWS Application Migration Service (MGN)
   - Plan migration projects by gathering information about on-premises data centers. Server utilization data and dependency mapping are important for migrations
   - Agentless Discovery (AWS Agentless Discovery Connector) / Agent-based Discovery (AWS Application Discovery Agent)
   - Resulting data can be viewed within AWS Migration Hub
   - Lift-and-shift (rehost) solution which simplify migrating applications to AWS
   - Converts your physical, virtual, and cloud-based servers to run natively on AWS (staging: low-cost EC2 instances & EBS volumes, then perform a cutover to switch to production: target EC2 instances and EBS volumes)
   - Supports wide range of platforms, Operating Systems, and databases
   - Minimal downtime, reduced costs
 - Transferring large amount of data into AWS
   - Example: transfer 200 TB of data in the cloud. We have a 100 Mbps internet connection.
   - 1)Over the internet / Site-to-Site VPN: quick setup, but takes very long time
   - 2)Over direct connect 1Gbps: long time to setup(more than 1 month), much quicker than the first way
   - 3)Over Snowball: Will take 2 to 3 snowballs in parallel and take about 1 week for e2e transfer, and can be combined with DMS
   - 4)For on-going replication / transfers: Site-to-Site VPN or DX with DMS or DataSync
 - VMware Cloud on AWS
   - Some customers use VMware Cloud to manage their on-premises Data Center. They want to extend the Data Center capacity to AWS, but keep using the VMware Cloud software. Enter VMware Cloud on AWS
   - use cases: Migrate your VMware vSphere-based workloads to AWS
   - Run your production workloads across VMware vSphere-based private, public, and hybrid cloud environments
   - Have a disaster recover strategy
## More Solutions Architecture
 - Event processing in AWS:
   - Lambda, SQS, SNS:
     - SQS ---> Lambda, then SQS send ---> DLQ
     - SQS FIFO ---> Lambda, then SQS send ---> DLQ 
     - SNS async---> Lambda(retry), then Lambda send DLQ ---> SQS
   - fan-out pattern:
     - SDK ---> SNS ---> SQS *n (subscribed)
   - S3 Event notifications : S3 send:events ---> SQS, SNS, Lambda
   - S3 Event notifications with EventBridge:
     - S3 send:events ---> EventBridge ---> over 18 AWS services as destinations
     - Advanced filtering options with JSON rules (metadata, object size, name...)
     - Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
     - EventBridge Capabilities – Archive, Replay Events, Reliable delivery
   - Amazon EventBridge – Intercept API Calls: CloudTrail will catch any API calls and send corresponding events to the EVentBridge, and EventBridge will send events or alerts to certain destination, such as SNS.
   - API Gateway – AWS Service Integration Kinesis Data Streams example (external events):
     - client send requests ---> API Gateway ---> Kinesis Data Stream ---> Kinesis Data Firehose ---> S3
 - Caching Strategies:
   - client ---> CloudFront ---> API Gateway ---> App logic (EC2 / Lambda) ---> Databases / (ElastiCache or DAX) // dynamic routes with dynamic contents
     - CloudFront(edge) ---> S3 // static contents
   - Caching, TTL, Network, Computation, Cost, latency
   - CloudFront: help cache contents at the edge location which is close to the end users (setup TTL in case of any change of the contents)
   - API Gateway (regional): cache between clients and API gateway
   - the backend logic doesn't have any cache capability, but the in-memory DB or DAX could help cache some complicated queries
   - there's no cache in S3 buckets or DBs.
 - Blocking an IP address
   - with NACL as well as security group, we can deny certain IP addresses
   - for NLB(fixed IP address), ALB, make sure security group for EC2 instances allow traffic from load balancers
   - for ALB, we can use WAF to do some more complex IP filtering
   - with CloudFront, we can restrict IPs from certain geo-locations, and also by using WAF, we can block certain IPs
 - High Performance Computing (HPC): quickly add resources within no time and pay only what you use, then what services help perform HPC?
   - Data Management & Transfer:
     - AWS Direct Connect: Move GB/s of data to the cloud, over a private secure network
     - Snowball & Snowmobile: Move PB of data to the cloud
     - AWS DataSync: Move large amount of data between on-premises and S3, EFS, FSx for Windows
   - Compute and Networking
     - EC2 Instances: CPU optimized, GPU optimized, Spot Instances / Spot Fleets for cost savings + Auto Scaling
     - EC2 Placement Groups: Cluster for good network performance
     - EC2 Enhanced Networking (SR-IOV)
       - Higher bandwidth, higher PPS (packet per second), lower latency
       - Option 1: **Elastic Network Adapter (ENA)** up to 100 Gbps
       - Option 2: Intel 82599 VF up to 10 Gbps – LEGACY
     - **Elastic Fabric Adapter (EFA)**
       - Improved ENA for HPC, only works for Linux
       - Great for inter-node communications, tightly coupled workloads
       - Leverages Message Passing Interface (MPI) standard
       - **Bypasses the underlying Linux OS to provide low-latency, reliable transport**
   - Storage
     - Instance-attached storage:
       - EBS: scale up to 256,000 IOPS with io2 Block Express
       - Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency
     - Network storage:
       - Amazon S3: large blob, not a file system
       - Amazon EFS: scale IOPS based on total size, or use provisioned IOPS
       - Amazon FSx for Lustre: HPC optimized distributed file system, millions of IOPS. Backed by S3
   - Automation and Orchestration
     - AWS Batch: AWS Batch supports multi-node parallel jobs, which enables you to run single jobs that span multiple EC2 instances. Easily schedule jobs and launch EC2 instances accordingly.
     - AWS ParallelCluster: Open-source cluster management tool to deploy HPC on AWS. Configure with text files. Automate creation of VPC, Subnet, cluster type and instance types. Ability to enable EFA on the cluster (improves network performance)
 - Creating a highly available EC2 instance
   - having a standby EC2 instance, cloudwatch and lambda (start the standby instance and attach it with the elastic IP)
   - using ASG, and user data attachment based on Tag for elastic IP
   - ASG + EBS, ASG terminate lifecycle hook: create an EBS snapshot with tag, then when a new instance launched in a new AZ, ASG launch lifecycle hook: create EBS volume from the snapshot.
## Other services
 - CloudFormation: a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported).
   - CloudFormation creates any resources for you based on your requirements, in the right order, with the exact configuration that you specify
   - Infrastructure as code
   - cost: Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely. Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you
   - Productivity: Ability to destroy and re-create an infrastructure on the cloud on the fly
   - Don’t re-invent the wheel: using exiting template or documentation
   - Supports (almost) all AWS resources: You can use “custom resources” for resources that are not supported
   - CloudFormation Stack Designer
 - Amazon Simple Email Service (Amazon SES): Fully managed service to send emails securely, globally and at scale
 - Amazon Pinpoint: Scalable 2-way (outbound/inbound) marketing communications service ( email, SMS, push, voice, and in-app messaging)
   - Versus Amazon SNS or Amazon SES: In Amazon Pinpoint, you create message templates, delivery schedules, highly-targeted segments, and full campaigns. In SNS & SES you managed each message's audience, content, and delivery schedule
 - Systems Manager – SSM Session Manager: Allows you to start a secure shell on your EC2 and on-premises servers (no need port 22, SSH access key, bastion host, just need IAM permission)
 - Systems Manager – Run Command: Execute a document (= script) or just run a command on multiple instances. no need SSH. Output sent to S3 or cloudwatch log. send notifications to SNS. can be invoke by EventBridge. Integrate with IAM and CloudTrail
 - Systems Manager – Patch Manager: Automates the process of patching managed instances. Support multiple OS and EC2 and on-prem. Patch on-demand or scheduled using **Maintenance Window**. Scan instances and generate patch compliance report (missing patches)
 - Systems Manager – Maintenance Windows: Defines a schedule for when to perform actions on your instances
   - schedule
   - duration
   - Set of registered instances
   - Set of registered tasks
 - Systems Manager - Automation
   - Simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources
   - Examples: restart instances, create an AMI, EBS snapshot
   - Automation Runbook – SSM Documents to define actions preformed on your EC2 instances or AWS resources (pre-defined or custom)
   - Can be triggered using: AWS Console, AWS CLI or SDK, EventBridge, On a schedule using Maintenance Windows, By AWS Config for rules remediations
 - Cost Explorer: Visualize, understand, and manage your AWS costs and usage over time (monthly,hourly...) 
   - Choose an optimal Savings Plan (to lower prices on your bill)
   - Forecast usage up to 12 months based on previous usage
 - Amazon Elastic Transcoder: Elastic Transcoder is used to convert media files stored in S3 into media files in the formats required by consumer playback devices (phones, etc..)
   - s3 input --> transcoder --> s3 output --> client
   - highly scalable, easy-to-use, cost-effective, fully managed&secure, pay what you use
 - AWS Batch: Fully managed batch processing at any scale(100,000s)
   - dynamically launch EC2 instances or Spot Instances
   - provisions the right amount of compute / memory
   - submit or schedule batch jobs and AWS Batch does the rest!
   - Batch jobs are defined as Docker images and run on ECS
   - Helpful for cost optimizations and focusing less on the infrastructure
 - AWS Batch vs Lambda(serverless, limited time, limited runtimes, Limited temporary disk space)
   - No time limit
   - Any runtime as long as it’s packaged as a Docker image
   - Rely on EBS / instance store for disk space
   - Relies on EC2 (can be managed by AWS)
 - Amazon AppFlow: Fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications and AWS
   - source: Salesforce, SAP, Zendesk, Slack, and ServiceNow
   - destination: AWS services like Amazon S3, Amazon Redshift or non-AWS such as SnowFlake and Salesforce
   - on-demand or schedule
   - data transformation
   - Encrypted over the public internet or privately over AWS PrivateLink (VPC endpoint)
   - Don’t spend time writing the integrations and leverage APIs immediately
 - AWS Amplify - web and mobile applications version of Elastic Beanstalk: A set of tools and services that helps you develop and deploy scalable full-stack web and mobile applications
## White Papers and Architectures
- Well-Architected Framework General Guiding Principles
  - Stop guessing your capacity needs
  - Test systems at production scale
  - Automate to make architectural experimentation easier
  - Allow for evolutionary architectures: Design based on changing requirements
  - Drive architectures using data
  - Improve through game days: Simulate applications for flash sale days
- Well-Architected Framework 6 Pillars
  - 1) Operational Excellence
  - 2) Security
  - 3) Reliability
  - 4) Performance Efficiency
  - 5) Cost Optimization
  - 6) Sustainability
  - 7) They are not something to balance, or trade-offs, they’re a synergy
- AWS Well-Architected Tool: Free tool to review your architectures against the 6 pillars Well-Architected Framework and adopt architectural best practices
- Trusted Advisor: high level AWS account assessment on 5 categories:
  - Cost optimization
  - Performance
  - Security
  - Fault tolerance
  - Service limits
- Trusted Advisor – Support Plans
  - Basic & Developer Support plan:7 CORE CHECKS
    - S3 Bucket Permissions
    - Security Groups – Specific Ports Unrestricted
    - IAM Use (one IAM user minimum)
    - MFA on Root Account
    - EBS Public Snapshots
    - RDS Public Snapshots
    - Service Limits
  - Business & Enterprise Support plan: Full Checks available on the 5 categories. Programmatic Access using AWS Support API. Ability to set CloudWatch alarms when
reaching limits

## Filling Gaps
 - Table of Contents
   - test-0
   - test-1
   - test-2:
     - RDS replicas can only allow `read` operations
     - RDS: multi-AZ synchronous, multi-region asynchronous (Aurora -- asynchronous)
     - transit gateway: AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway.
     - spread placement group: limits: 7 instances per AZ per group, meaning 21 instances for 3 AZ
     - Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Amazon Kinesis Data Streams or Amazon Kinesis Firehose.
     - use a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation. the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone.
     - route53: Hosted Zones: A container for records that define how to route traffic to a domain and its subdomains
       - Public Hosted Zones
       - Private Hosted Zones
       - Zone File: contains DNS records
       - DNS hostnames and DNS resolution are required settings for private hosted zones.
     - AWS Global Accelerator: uses `endpoint weights` to determine the proportion of traffic that is directed to endpoints in an endpoint group, and `traffic dials` to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed). Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment.
     - In blue/green deployment, blue for current version, green for new version. and this type of deployment can mitigate common risks associated with deploying software, such as downtime and rollback capability. 
     - Set the `DeleteOnTermination` attribute to false for EBS volume of EC2 instance
     - AWS WAF: gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.
     - Enable storage auto-scaling for Amazon RDS MySQL: If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage.
     - Amazon RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the Amazon RDS instance is located. Amazon RDS stores these on your behalf and you do not have direct access to these snapshots in Amazon S3, so it's not possible to grant access to the snapshot objects in Amazon S3. While making an encrypted snapshot of the database will give the auditor a copy of the database
     - AWS Direct Connect provides three types of virtual interfaces: public, private, and transit.
     - AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.
     - However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. So this is the correct option.
     - you cannot convert an existing single-Region key to a multi-Region key (KMS single-region key & multi-region key), so that you must create a new bucket in the same region as the current bucket, and also enable replication in a new region bucket, as well as multi-region key on this new bucket. then copy the existing data from the current bucket into this new bucket.
     - Aurora Replicas have two main purposes. to scale the read operations, Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.
     - There are data transfer charges for replicating data across AWS Regions
     - KMS is an encryption service, it's not a secrets store. AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.
     - `The Spot Fleet` selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.
     - A launch template is similar to a launch configuration, in that it specifies instance configuration information such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances. Also, defining a launch template instead of a launch configuration allows you to have multiple versions of a template. With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost. Hence this is the correct option.
     - use an IAM role to manage temporary credentials for applications that run on an Amazon EC2 instance.
     - An Amazon EventBridge expiry event is published for any certificate that is at least 45 days away from expiry by default.
     - an AWS Config managed rule to check if any ACM certificates in your account are marked for expiration within the specified number of days(30 days for example). Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. The rule is NON_COMPLIANT if your certificates are about to expire.
     - If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy.
     - EC2 ASG termination policy: Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority. (ASG will terminate the oldest launch configuration before the oldest launch template)
     - a popular approach is to build a "shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost. Centralized VPC Endpoints (multiple VPCs): build a shared vpc around transit gateway with necessary resources and vpc endpoints
     - Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires.
     - Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard.
     - Amazon EFS is a `file` storage service . Amazon S3 is an `object` storage service.
     - `aws:RequestedRegion` represents the target of the API call. `aws:SourceIp` restrict the client IP from which the API calls are being made. `ec2:ResourceTag` restrict based on tags. `aws:MultiFactorAuthPresent` to force MFA. `aws:PrincipalOrgID` can be used in any resource policies to restrict access to accounts that are member of an AWS Organization.
     - AWS supports permissions boundaries for IAM entities (users or roles not groups). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity.
     - If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most. To allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the "Group ID" attribute. So this is the correct option. **note: **A Kinesis Data Stream would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers).
     - AWS does not offer the multi-master feature in a Aurora database cluster.
     - Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications.
     - AWS CloudFormation takes time to provision the resources and hence is not the right solution when LEAST amount of downtime is mandated for the given use case.
     - S3 access control:
       - bucket policy: can be used at user level or aws account level
       - iam permission: can only be used at user level within your aws account
       - ACL(access control list): can only be used at aws account level
     - ALB can be integrated with AWS Cognito
     - Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ). Even if one of the AZs goes out of service, still we shall have 4 instances available and the application can maintain an acceptable level of end-user experience. Therefore, we can achieve high availability with just 6 instances in this case.
     - From a cost perspective, when using a zero-day lifecycle policy, you are only charged Amazon S3 Glacier Deep Archive rates.
     - By default, an Amazon S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. create a cluster Role and a bucket Role, make trust relationship between the two.
     - AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge. The correct solution is to share the subnet(s) within a VPC using RAM. This will allow all Amazon EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another.
     - EFS mode:
       - performance mode: general purpose , Max I/O : big data, media processing
       - Throughput Mode: bursting, provisioned, elastic
   - test-3
     - AWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an "AWS Organization", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an "AWS Organization" across specified regions. AWS CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions.
     - if the data set is less than 1GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance. The given use case has data larger than 1GB and hence S3 Transfer Acceleration is a better option.
     - for RDS cross-region read replicas, promote a replica to a primary DB during failover is not automatic, hence the RTO is high. while for dynamodb global tables, there's no failover because of active-active configuration, the data is replicated to keep the other regions' table in sync which is more costier than Aurora global tables which has one primary region and up to 5 secondary (read-only) regions and has two types of failovers: managed planned failover and unplanned failover.
     - database migration from RDS mysql to Aurora mysql would improve performance and availability with more read replicas and no virtual replication lag.
     - You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO). AWS Managed Microsoft AD is your best choice if you have more than 5,000 users and need a trust relationship set up between an AWS hosted directory and your on-premises directories.
     - You should also note that Amazon Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries. Additionally, an alias record can only redirect queries to selected AWS resources such as Amazon S3 buckets, Amazon CloudFront distributions, and another record in the same Amazon Route 53 hosted zone; however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.
     - KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. Amazon Kinesis Data Streams stores data for a maximum of 365 days
     - source or destination for security group rules: ipv4 / ipv6/ prefix list ID for aws services / another security group
     - **If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance. If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target.**
     - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.
     - `An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. It enhances the performance of inter-instance communication that is critical for scaling HPC and machine learning applications. EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality.`
     - Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects
     - Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution
     - Amazon ElastiCache can be used to significantly improve latency and throughput for many `read-heavy` application workloads (such as social networking, gaming, media sharing, leaderboard, and Q&A portals) or `compute-intensive` workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.
     - VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as Amazon EC2 instances, Amazon RDS databases, Amazon Redshift clusters, and AWS Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations.
     - To ensure that Elastic Load Balancing stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the de-registering instance.
     - `You can only change the tenancy of an instance from dedicated( dedicated to a single customer, may share hardware with other instances in the same aws account that's not dedicated) to host(you have visibility and control over how instances are placed on the server.), or from host to dedicated after you've launched it. By default, Amazon EC2 instances run on a shared-tenancy basis(on the shared hardware). `
     - If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.
     - AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.
     - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. With cached volumes, the AWS Volume Gateway stores the full volume in its Amazon S3 service bucket, and just the recently accessed data is retained in the gateway’s local cache for low-latency access. With stored volumes, your entire data volume is available locally in the gateway, for fast read access.
   - test-4
   - test-5
   - test-6


